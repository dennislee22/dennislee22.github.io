ocpbastion:

openssl crl2pkcs7 -nocrl -certfile /etc/pki/tls/certs/ca-bundle.crt | openssl pkcs7 -print_certs | grep subject > pkiorig

openssl crl2pkcs7 -nocrl -certfile /etc/pki/tls/certs/ca-bundle.crt | openssl pkcs7 -print_certs | grep subject | wc -l
127


[root@ocpbastion ~]# oc get clusterversions.config.openshift.io
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.7.53    True        False         12m     Cluster version is 4.7.53

[root@ocpbastion ~]# oc get nodes
NAME                        STATUS   ROLES           AGE   VERSION
master01.ocp4.cdpkvm.cldr   Ready    master,worker   31m   v1.20.15+98b2293
master02.ocp4.cdpkvm.cldr   Ready    master,worker   31m   v1.20.15+98b2293
master03.ocp4.cdpkvm.cldr   Ready    master,worker   31m   v1.20.15+98b2293

[root@ocpbastion ~]# oc get co
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             4.7.53    True        False         False      14m
baremetal                                  4.7.53    True        False         False      28m
cloud-credential                           4.7.53    True        False         False      39m
cluster-autoscaler                         4.7.53    True        False         False      28m
config-operator                            4.7.53    True        False         False      30m
console                                    4.7.53    True        False         False      19m
csi-snapshot-controller                    4.7.53    True        False         False      24m
dns                                        4.7.53    True        False         False      28m
etcd                                       4.7.53    True        False         False      28m
image-registry                             4.7.53    True        False         False      24m
ingress                                    4.7.53    True        False         False      26m
insights                                   4.7.53    True        False         False      23m
kube-apiserver                             4.7.53    True        False         False      27m
kube-controller-manager                    4.7.53    True        False         False      27m
kube-scheduler                             4.7.53    True        False         False      28m
kube-storage-version-migrator              4.7.53    True        False         False      29m
machine-api                                4.7.53    True        False         False      29m
machine-approver                           4.7.53    True        False         False      29m
machine-config                             4.7.53    True        False         False      29m
marketplace                                4.7.53    True        False         False      29m
monitoring                                 4.7.53    True        False         False      23m
network                                    4.7.53    True        False         False      29m
node-tuning                                4.7.53    True        False         False      29m
openshift-apiserver                        4.7.53    True        False         False      24m
openshift-controller-manager               4.7.53    True        False         False      27m
openshift-samples                          4.7.53    True        False         False      23m
operator-lifecycle-manager                 4.7.53    True        False         False      29m
operator-lifecycle-manager-catalog         4.7.53    True        False         False      29m
operator-lifecycle-manager-packageserver   4.7.53    True        False         False      24m
service-ca                                 4.7.53    True        False         False      30m
storage                                    4.7.53    True        False         False      30m


[root@ocpbastion ~]# oc get pods -n openshift-ingress
NAME                              READY   STATUS    RESTARTS   AGE
router-default-5b7459c989-fcf4w   1/1     Running   0          91m
router-default-5b7459c989-jvhpr   1/1     Running   0          91m


[root@ocpbastion ~]# oc describe pod router-default-5b7459c989-fcf4w -n openshift-ingress | grep -i "Node:"
Node:                 master01.ocp4.cdpkvm.cldr/10.15.4.182
[root@ocpbastion ~]# oc describe pod router-default-5b7459c989-jvhpr -n openshift-ingress | grep -i "Node:"
Node:                 master02.ocp4.cdpkvm.cldr/10.15.4.183


[root@ocpbastion ~]# oc debug node/master01.ocp4.cdpkvm.cldr
Starting pod/master01ocp4cdpkvmcldr-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.15.4.182
If you don't see a command prompt, try pressing enter.
sh-4.4# chroot /host
sh-4.4# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0  100G  0 disk 
|-vda1 252:1    0    1M  0 part 
|-vda2 252:2    0  127M  0 part 
|-vda3 252:3    0  384M  0 part /boot
`-vda4 252:4    0 99.5G  0 part /sysroot
vdb    252:16   0  250G  0 disk 
vdc    252:32   0  400G  0 disk 


[root@ocpbastion ~]# oc get nodes -l cluster.ocs.openshift.io/openshift-storage=
NAME                        STATUS   ROLES           AGE     VERSION
master01.ocp4.cdpkvm.cldr   Ready    master,worker   4h31m   v1.20.15+98b2293
master02.ocp4.cdpkvm.cldr   Ready    master,worker   4h31m   v1.20.15+98b2293
master03.ocp4.cdpkvm.cldr   Ready    master,worker   4h31m   v1.20.15+98b2293





[root@ocpbastion ~]# oc get sc
NAME                                    PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
cdw-disk                                kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  22m
ocs-storagecluster-ceph-rbd (default)   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   40m
ocs-storagecluster-ceph-rgw             openshift-storage.ceph.rook.io/bucket   Delete          Immediate              false                  40m
ocs-storagecluster-cephfs               openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   40m
openshift-storage.noobaa.io             openshift-storage.noobaa.io/obc         Delete          Immediate              false                  36m
vdb                                     kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  42m




[root@ocpbastion ~]# oc get pods -n cdp | head
NAME                                                              READY   STATUS                  RESTARTS   AGE
cdp-release-alert-admin-service-957bfcb58-hz8g7                   0/2     ImagePullBackOff        0          77s
cdp-release-classic-clusters-7df4fb5ccb-94bsl                     0/3     Init:ImagePullBackOff   0          71s
cdp-release-cluster-access-manager-57447b4895-bnfc7               0/2     ImagePullBackOff        0          76s
cdp-release-cluster-proxy-1.0.0-67b59bd449-nz8pb                  0/2     ErrImagePull            0          72s
cdp-release-cpx-liftie-7dccf746b7-85djb                           0/2     ImagePullBackOff        0          82s
cdp-release-dex-cp-74788c5f79-t6cqt                               0/2     ImagePullBackOff        0          79s
cdp-release-dmx-55bbfd8cd9-skkg6                                  0/3     ImagePullBackOff        0          71s
cdp-release-dps-gateway-1.0-5d58765b8-6p4ff                       0/3     ImagePullBackOff        0          87s
cdp-release-dps-gateway-1.0-5d58765b8-jbfqj                       0/3     ImagePullBackOff        0          87s
[root@ocpbastion ~]# oc describe pod cdp-release-alert-admin-service-957bfcb58-hz8g7 -n cdp | tail
  Normal   Pulling         62s (x2 over 78s)  kubelet            Pulling image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera_thirdparty/fluent-bit:v1.4.6-3896242"
  Warning  Failed          62s (x2 over 78s)  kubelet            Failed to pull image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera/monitoring-app:1.3.5-b30": rpc error: code = Unknown desc = error pinging docker registry nexus.cdpkvm.cldr:9999: Get "https://nexus.cdpkvm.cldr:9999/v2/": x509: certificate signed by unknown authority
  Warning  Failed          62s (x2 over 78s)  kubelet            Error: ErrImagePull
  Normal   Pulling         62s (x2 over 78s)  kubelet            Pulling image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera/monitoring-app:1.3.5-b30"
  Warning  Failed          62s (x2 over 78s)  kubelet            Failed to pull image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera_thirdparty/fluent-bit:v1.4.6-3896242": rpc error: code = Unknown desc = error pinging docker registry nexus.cdpkvm.cldr:9999: Get "https://nexus.cdpkvm.cldr:9999/v2/": x509: certificate signed by unknown authority
  Warning  Failed          62s (x2 over 78s)  kubelet            Error: ErrImagePull
  Normal   BackOff         49s (x3 over 78s)  kubelet            Back-off pulling image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera/monitoring-app:1.3.5-b30"
  Warning  Failed          49s (x3 over 78s)  kubelet            Error: ImagePullBackOff
  Normal   BackOff         49s (x3 over 78s)  kubelet            Back-off pulling image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera_thirdparty/fluent-bit:v1.4.6-3896242"
  Warning  Failed          49s (x3 over 78s)  kubelet            Error: ImagePullBackOff
  
  
https://docs.openshift.com/container-platform/4.7/cicd/builds/setting-up-trusted-ca.html


[root@ocpbastion ~]# oc create configmap registry-cas -n openshift-config --from-file=nexus.cdpkvm.cldr..9999=/root/nexus.crt 
configmap/registry-cas created
[root@ocpbastion ~]# oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-cas"}}}' --type=merge
image.config.openshift.io/cluster patched


[root@ocpbastion ~]# oc get pvc
NAME                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
cdp-release-prometheus-server                          Bound    pvc-63b9d337-d7c1-4051-b4c5-e46949affd0d   10Gi       RWO            ocs-storagecluster-ceph-rbd   23m
logs                                                   Bound    pvc-2c623b62-e90b-40b2-92c5-e4343528885a   20Gi       RWO            ocs-storagecluster-ceph-rbd   23m
storage-volume-cdp-release-prometheus-alertmanager-0   Bound    pvc-e3e0ac29-70b1-445e-80b3-b7a92a8f1923   2Gi        RWO            ocs-storagecluster-ceph-rbd   23m
storage-volume-cdp-release-prometheus-alertmanager-1   Bound    pvc-93b032f5-2115-4c92-b77c-940458d39412   2Gi        RWO            ocs-storagecluster-ceph-rbd   10m


CDW:

[root@ocpbastion ~]# oc -n shared-services get pods
NAME                                    READY   STATUS    RESTARTS   AGE
log-router-g787n                        2/2     Running   0          55s
log-router-q8j6c                        2/2     Running   0          55s
log-router-xfrbt                        2/2     Running   0          55s
openshift-idling-controller-manager-0   1/1     Running   0          54s




https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.6/html/deploying_openshift_container_storage_using_bare_metal_infrastructure/deploy-using-local-storage-devices-bm#installing-local-storage-operator_rhocs

[root@ocpbastion ~]# oc -n warehouse-1656305490-mgc9 get pvc
No resources found in warehouse-1656305490-mgc9 namespace.
[root@ocpbastion ~]# oc -n warehouse-1656305490-mgc9 get pods
NAME                    READY   STATUS    RESTARTS   AGE
das-event-processor-0   1/1     Running   0          8m23s
metastore-0             1/1     Running   0          8m23s
metastore-1             1/1     Running   0          5m54s



[root@ocpbastion ~]# oc get sc
NAME                                    PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
cdw-disk                                kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  2m9s
ocs-storagecluster-ceph-rbd (default)   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   24h
ocs-storagecluster-ceph-rgw             openshift-storage.ceph.rook.io/bucket   Delete          Immediate              false                  24h
ocs-storagecluster-cephfs               openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   24h
openshift-storage.noobaa.io             openshift-storage.noobaa.io/obc         Delete          Immediate              false                  24h
vdb                                     kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  24h

[root@ocpbastion ~]# oc get pv | head -1; oc get pv | grep local
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                                        STORAGECLASS                  REASON   AGE
local-pv-12e7ab5                           400Gi      RWO            Delete           Available                                                                                                cdw-disk                               2m33s
local-pv-17bf8fbc                          300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-2slfmj                                            vdb                                    24h
local-pv-33a6a00a                          400Gi      RWO            Delete           Available                                                                                                cdw-disk                               2m34s
local-pv-62cdcca                           300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-06dblw                                            vdb                                    24h
local-pv-abb1e063                          400Gi      RWO            Delete           Available                                                                                                cdw-disk                               2m34s
local-pv-aca516bf                          300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-1t74g8                                            vdb                                    24h


[root@ocpbastion ~]# oc describe pv local-pv-12e7ab5
Name:              local-pv-12e7ab5
Labels:            storage.openshift.com/local-volume-owner-name=cdw-localdisk-each-worker
                   storage.openshift.com/local-volume-owner-namespace=openshift-local-storage
Annotations:       pv.kubernetes.io/provisioned-by: local-volume-provisioner-master01.ocp4.cdpkvm.cldr-4603dd1e-328f-46b2-b086-48f98ec5d9e7
Finalizers:        [kubernetes.io/pv-protection]
StorageClass:      cdw-disk
Status:            Available
Claim:             
Reclaim Policy:    Delete
Access Modes:      RWO
VolumeMode:        Filesystem
Capacity:          400Gi
Node Affinity:     
  Required Terms:  
    Term 0:        kubernetes.io/hostname in [master01.ocp4.cdpkvm.cldr]
Message:           
Source:
    Type:  LocalVolume (a persistent volume backed by local storage on a node)
    Path:  /mnt/local-storage/cdw-disk/vdc
Events:    <none>




provision hive (1 executor):

[root@ocpbastion ~]# oc -n compute-1656330332-ph2j get pods
NAME                             READY   STATUS    RESTARTS   AGE
das-webapp-0                     1/1     Running   0          2m43s
hiveserver2-0                    1/1     Running   0          2m43s
huebackend-0                     1/1     Running   0          2m43s
huefrontend-6bcbb8fdfb-8kb7s     1/1     Running   0          2m43s
query-coordinator-0-0            1/1     Running   0          2m32s
query-executor-0-0               1/1     Running   0          2m32s
standalone-compute-operator-0    1/1     Running   0          2m43s
usage-monitor-564fdcdbcc-mbgmc   1/1     Running   0          2m43s

[root@ocpbastion ~]# oc -n compute-1656330332-ph2j get pvc
NAME                                                  STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
query-executor-1656330358-volume-query-executor-0-0   Bound    local-pv-abb1e063   400Gi      RWO            cdw-disk       2m34s


[root@ocpbastion ~]# oc -n compute-1656330332-ph2j describe pod query-executor-0-0 | grep Node:
Node:         master03.ocp4.cdpkvm.cldr/10.15.4.184

[root@ocpbastion ~]# oc describe pv local-pv-abb1e063 | grep master
                   pv.kubernetes.io/provisioned-by: local-volume-provisioner-master03.ocp4.cdpkvm.cldr-43054c69-a851-47c4-a3c3-b7d2b74d8a2e
    Term 0:        kubernetes.io/hostname in [master03.ocp4.cdpkvm.cldr]

[root@ocpbastion ~]# oc describe pv local-pv-abb1e063 | grep hostname
    Term 0:        kubernetes.io/hostname in [master03.ocp4.cdpkvm.cldr]
    
    
provision impala (1 executor):

[root@ocpbastion ~]# oc -n impala-1656330683-rm4v get pods
NAME                                 READY   STATUS    RESTARTS   AGE
catalogd-74869dbf89-ldrmh            1/1     Running   0          2m2s
coordinator-0                        4/4     Running   0          2m2s
coordinator-1                        3/4     Running   0          80s
huebackend-0                         2/2     Running   0          2m2s
huefrontend-74c45775c-zz2lv          1/1     Running   0          2m2s
impala-autoscaler-5b484b9fb7-znsxw   1/1     Running   0          2m1s
impala-executor-000-0                1/1     Running   0          2m2s
statestored-694d8b5fbb-529tp         1/1     Running   0          2m2s
usage-monitor-755869884-ww6cn        1/1     Running   0          2m2s

[root@ocpbastion ~]# oc -n impala-1656330683-rm4v get pvc
NAME                                         STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
scratch-cache-volume-coordinator-0           Bound    local-pv-abb1e063   400Gi      RWO            cdw-disk       9s
scratch-cache-volume-impala-executor-000-0   Bound    local-pv-33a6a00a   400Gi      RWO            cdw-disk       9s
[root@ocpbastion ~]# oc ^C
[root@ocpbastion ~]# oc -n impala-1656330683-rm4v describe pod coordinator-0 | grep Node:
Node:         master03.ocp4.cdpkvm.cldr/10.15.4.184
[root@ocpbastion ~]# oc describe pv local-pv-abb1e063 | grep hostname
    Term 0:        kubernetes.io/hostname in [master03.ocp4.cdpkvm.cldr]
[root@ocpbastion ~]# oc -n impala-1656330683-rm4v describe pod impala-executor-000-0 | grep Node:
Node:         master02.ocp4.cdpkvm.cldr/10.15.4.183
[root@ocpbastion ~]# oc describe pv local-pv-33a6a00a | grep hostname
    Term 0:        kubernetes.io/hostname in [master02.ocp4.cdpkvm.cldr]
[root@ocpbastion ~]# oc get sc | head -1 ; oc get sc | grep cdw-disk
NAME                                    PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
cdw-disk                                kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  53s
[root@ocpbastion ~]# ssh core@master03.ocp4.cdpkvm.cldr
Red Hat Enterprise Linux CoreOS 47.84.202206080457-0
  Part of OpenShift 4.7, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.7/architecture/architecture-rhcos.html

---
Last login: Mon Jun 27 11:27:34 2022 from 10.15.4.189
[core@master03 ~]$ ll /mnt/local-storage/
total 0
drwxr-xr-x. 2 root root 17 Jun 27 08:41 cdw-disk
drwxr-xr-x. 2 root root 17 Jun 26 15:18 vdb
[core@master03 ~]$ ll /mnt/local-storage/cdw-disk
total 0
lrwxrwxrwx. 1 root root 8 Jun 27 08:41 vdc -> /dev/vdc                                             cdw-disk                               11m


[root@ocpbastion ~]# oc get sc | head -1 ; oc get sc | grep cdw-disk
NAME                                    PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
cdw-disk                                kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  53s


Add new disk in cdw-disk in local storage operator


[root@ocpbastion ~]# ssh core@master02.ocp4.cdpkvm.cldr
Red Hat Enterprise Linux CoreOS 47.84.202206080457-0
  Part of OpenShift 4.7, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.7/architecture/architecture-rhcos.html

---
Last login: Mon Jun 27 11:27:22 2022 from 10.15.4.189
[core@master02 ~]$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0    7:0    0  300G  0 loop 
rbd0   251:0    0    2G  0 disk /var/lib/kubelet/pods/0bd4e204-3a6f-457f-8b05-68b4ae4b59a5/volumes/kubernetes.io~csi/pvc-8dfd9e8b-4ce5-4b9d-aa35-04ef516ca124/
rbd1   251:16   0    2G  0 disk /var/lib/kubelet/pods/bc1f562c-de29-48d8-92d5-55e73db4cbf3/volumes/kubernetes.io~csi/pvc-1aa5ade9-5ace-4058-a074-957abf8aaaaa/
rbd2   251:32   0   20G  0 disk /var/lib/kubelet/pods/e04b26fe-229b-4fec-b056-4cd67c9950a1/volumes/kubernetes.io~csi/pvc-cfa677f6-5fd3-4d96-a5b5-dda4162a36f5/
vda    252:0    0  100G  0 disk 
|-vda1 252:1    0    1M  0 part 
|-vda2 252:2    0  127M  0 part 
|-vda3 252:3    0  384M  0 part /boot
`-vda4 252:4    0 99.5G  0 part /sysroot
vdb    252:16   0  300G  0 disk 
vdc    252:32   0  400G  0 disk 
vdd    252:48   0  200G  0 disk 
[core@master02 ~]$ ll /mnt/local-storage/
total 0
drwxr-xr-x. 2 root root 17 Jun 27 08:41 cdw-disk
drwxr-xr-x. 2 root root 17 Jun 26 15:17 vdb
[core@master02 ~]$ ll /mnt/local-storage/cdw-disk/
total 0
lrwxrwxrwx. 1 root root 8 Jun 27 08:41 vdc -> /dev/vdc


[core@master02 ~]$ ll /mnt/local-storage/cdw-disk/
total 0
lrwxrwxrwx. 1 root root 8 Jun 27 08:41 vdc -> /dev/vdc
lrwxrwxrwx. 1 root root 8 Jun 27 12:00 vdd -> /dev/vdd

[root@ocpbastion ~]# oc get pv | head -1; oc get pv | grep local
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                                        STORAGECLASS                  REASON   AGE
local-pv-12e7ab5                           400Gi      RWO            Delete           Available                                                                                                cdw-disk                               5m59s
local-pv-17bf8fbc                          300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-2slfmj                                            vdb                                    25h
local-pv-33a6a00a                          400Gi      RWO            Delete           Available                                                                                                cdw-disk                               6m40s
local-pv-62cdcca                           300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-06dblw                                            vdb                                    25h
local-pv-abb1e063                          400Gi      RWO            Delete           Available                                                                                                cdw-disk                               6m10s
local-pv-aca516bf                          300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-1t74g8                                            vdb                                    25h
local-pv-c8fe6eea                          200Gi      RWO            Delete           Available                                                                                                cdw-disk                               85s
local-pv-d30bd8ac                          200Gi      RWO            Delete           Available                                                                                                cdw-disk                               77s
local-pv-dc533915                          200Gi      RWO            Delete           Available                                                                                                cdw-disk                               66s


impala 4 executor:

[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 get pods
NAME                                READY   STATUS    RESTARTS   AGE
catalogd-567f684bcb-4dxph           1/1     Running   0          80s
coordinator-0                       4/4     Running   0          80s
huebackend-0                        2/2     Running   0          80s
huefrontend-7c765bf44d-47dkq        1/1     Running   0          80s
impala-autoscaler-6c7564d7f-lpndc   1/1     Running   0          80s
impala-executor-000-0               1/1     Running   0          80s
impala-executor-000-1               1/1     Running   0          80s
impala-executor-000-2               1/1     Running   0          80s
impala-executor-000-3               1/1     Running   0          80s
statestored-f9d6b4bb7-hhk65         1/1     Running   0          80s
usage-monitor-8677878c4b-q5hx8      1/1     Running   0          80s

[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 get pvc
NAME                                         STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
scratch-cache-volume-coordinator-0           Bound    local-pv-abb1e063   400Gi      RWO            cdw-disk       103s
scratch-cache-volume-impala-executor-000-0   Bound    local-pv-d30bd8ac   200Gi      RWO            cdw-disk       103s
scratch-cache-volume-impala-executor-000-1   Bound    local-pv-dc533915   200Gi      RWO            cdw-disk       103s
scratch-cache-volume-impala-executor-000-2   Bound    local-pv-c8fe6eea   200Gi      RWO            cdw-disk       103s
scratch-cache-volume-impala-executor-000-3   Bound    local-pv-33a6a00a   400Gi      RWO            cdw-disk       103s


[root@ocpbastion ~]# oc describe pv local-pv-33a6a00a | grep hostname
    Term 0:        kubernetes.io/hostname in [master02.ocp4.cdpkvm.cldr]

[root@ocpbastion ~]# oc describe pv local-pv-dc533915 | grep hostname
    Term 0:        kubernetes.io/hostname in [master02.ocp4.cdpkvm.cldr]
    
[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 describe pod impala-executor-000-1 | grep Node:
Node:         master02.ocp4.cdpkvm.cldr/10.15.4.183

[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 describe pod impala-executor-000-3 | grep Node:
Node:         master02.ocp4.cdpkvm.cldr/10.15.4.183


[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 exec -ti impala-executor-000-1 -- /bin/sh
sh-4.2$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
rbd0   251:0    0    2G  0 disk 
vdd    252:48   0  200G  0 disk /opt/impala/scratch
vdb    252:16   0  300G  0 disk 
rbd1   251:16   0    2G  0 disk 
loop0    7:0    0  300G  0 loop 
vdc    252:32   0  400G  0 disk 
vda    252:0    0  100G  0 disk 
|-vda4 252:4    0 99.5G  0 part /opt/impala/logs/minidumps
|-vda2 252:2    0  127M  0 part 
|-vda3 252:3    0  384M  0 part 
`-vda1 252:1    0    1M  0 part 
rbd2   251:32   0   20G  0 disk 

sh-4.2$ ls -l /opt/impala/scratch/
total 0
-rw-r--r--. 1 hive 1000800000 0 Jun 27 12:35 impala-cache-file-524e9102ab341ef1:1917da93c3905ab4
drwxr-sr-x. 2 hive 1000800000 6 Jun 27 12:35 impala-scratch


[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 exec -ti impala-executor-000-3 -- /bin/sh
sh-4.2$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
rbd0   251:0    0    2G  0 disk 
vdd    252:48   0  200G  0 disk 
vdb    252:16   0  300G  0 disk 
rbd1   251:16   0    2G  0 disk 
loop0    7:0    0  300G  0 loop 
vdc    252:32   0  400G  0 disk /opt/impala/scratch
vda    252:0    0  100G  0 disk 
|-vda4 252:4    0 99.5G  0 part /opt/impala/logs/minidumps
|-vda2 252:2    0  127M  0 part 
|-vda3 252:3    0  384M  0 part 
`-vda1 252:1    0    1M  0 part 
rbd2   251:32   0   20G  0 disk 
sh-4.2$ ls -l /opt/impala/scratch
total 0
-rw-r--r--. 1 hive 1000800000 0 Jun 27 12:46 impala-cache-file-154c4655952ade0d:c400e0289decbc93
drwxr-sr-x. 2 hive 1000800000 6 Jun 27 12:46 impala-scratch



ocpcml:

[root@ocpbastion ~]# oc get pods
NAME                                             READY   STATUS      RESTARTS   AGE
api-75f7d749f-8r72d                              1/1     Running     0          6m54s
cron-7bbfc48b5b-svxz2                            2/2     Running     0          6m54s
db-0                                             2/2     Running     0          6m54s
db-migrate-2.0.28-b66-9nj26                      0/1     Completed   0          6m44s
ds-cdh-client-b8dd5d99-f5c7f                     3/3     Running     0          6m54s
ds-operator-6f4c4c8957-wjjtk                     2/2     Running     1          6m54s
ds-reconciler-7fccf5cbd9-nfcjv                   2/2     Running     0          6m54s
ds-vfs-689c467d96-tttxv                          2/2     Running     0          6m54s
feature-flags-69447f8575-2jjzz                   2/2     Running     0          6m54s
fluentd-forwarder-5d855989fc-dxxvl               1/1     Running     0          6m54s
hadoop-cli-7.2.10-hf1-3z0poka-bwjqt              0/1     Completed   0          4m46s
hadoop-cli-7.2.11-hf4-62chh-sj7v8                0/1     Completed   0          4m46s
hadoop-cli-7.2.8-hf1-v6yy0j-z7lpj                0/1     Completed   0          4m46s
livelog-0                                        2/2     Running     0          6m54s
livelog-publisher-8nptf                          2/2     Running     0          6m54s
livelog-publisher-jb5ph                          2/2     Running     0          6m54s
livelog-publisher-m87d9                          2/2     Running     0          6m54s
model-metrics-77bd8c6575-f4j8w                   1/1     Running     3          6m54s
model-metrics-db-0                               1/1     Running     0          6m54s
model-proxy-f5ff968cd-czv2k                      2/2     Running     0          6m54s
prometheus-postgres-exporter-6f785b7755-6j68b    1/1     Running     0          6m54s
runtime-addon-trigger-2.0.28-b66-txfcj           0/1     Completed   0          6m44s
runtime-initial-repo-inserter-2.0.28-b66-j2psv   0/1     Completed   0          6m44s
runtime-manager-7775c48879-9kwgt                 2/2     Running     0          6m54s
s2i-builder-7f4879fb78-22qgv                     2/2     Running     0          6m54s
s2i-builder-7f4879fb78-nd7rs                     2/2     Running     0          6m54s
s2i-builder-7f4879fb78-z4zqt                     2/2     Running     0          6m54s
s2i-client-6f596848d8-kbkjb                      2/2     Running     0          6m54s
s2i-git-server-0                                 2/2     Running     0          6m54s
s2i-queue-0                                      2/2     Running     0          6m54s
s2i-registry-auth-7556777cbf-7nfpf               2/2     Running     0          6m54s
s2i-registry-c6787ccd-6rh4p                      2/2     Running     0          6m54s
s2i-server-bb479d45b-h8ljs                       2/2     Running     0          6m54s
secret-generator-0                               2/2     Running     0          6m54s
spark247-13-hf2-nvcpz-p4bmw                      0/1     Completed   0          4m46s
spark311-13-hf2-ft7g6a-nqtd6                     0/1     Completed   0          4m46s
tcp-ingress-controller-69c976d9d9-ctpsw          2/2     Running     0          6m54s
usage-reporter-6f67b66bd9-mm5df                  2/2     Running     0          6m54s
web-66476566f9-2jjs5                             2/2     Running     0          6m54s
web-66476566f9-n6lrc                             2/2     Running     0          6m54s
web-66476566f9-t5dxg                             2/2     Running     0          6m54s
[root@ocpbastion ~]# oc get pvc
NAME                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
livelog-data-livelog-0                  Bound    pvc-4054e760-535c-4667-8820-6bbedde65022   1Ti        RWO            ocs-storagecluster-ceph-rbd   7m2s
model-metrics-data-model-metrics-db-0   Bound    pvc-0b96f2d7-5341-4d55-8992-a4ced2aba6bb   100Gi      RWO            ocs-storagecluster-ceph-rbd   7m2s
persist-dir-secret-generator-0          Bound    pvc-9a40c7cb-649b-4453-b377-e98f65316b3b   10Mi       RWO            ocs-storagecluster-ceph-rbd   7m2s
postgres-data-versioned-db-0            Bound    pvc-c7142527-fcbf-47dd-b29e-570edee3bf2b   1Ti        RWO            ocs-storagecluster-ceph-rbd   7m2s
projects-pvc                            Bound    projects-share-ws1                         1Ti        RWX                                          7m3s
registry-pvc                            Bound    pvc-97381aea-f0b6-42a2-9638-883f9cc1fb5f   1Ti        RWO            ocs-storagecluster-ceph-rbd   7m3s
s2i-git-server-repos-s2i-git-server-0   Bound    pvc-20671830-671c-4c76-9538-1ea5d083058a   1Ti        RWO            ocs-storagecluster-ceph-rbd   7m2s
s2i-queue-pvc                           Bound    pvc-ee89ea8f-7a99-4f7b-9995-03c49b9a1e0c   2Gi        RWO            ocs-storagecluster-ceph-rbd   7m3s


CDE:

[root@ocpbastion ~]# oc -n dex-base-hqxd5d59 get pvc
NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
dex-base-db-pvc    Bound    pvc-55f1024a-c2b0-4a73-8370-1413ac7ac4cf   100Gi      RWO            ocs-storagecluster-ceph-rbd   95s
dex-base-grafana   Bound    pvc-9fda6a03-06bd-4f8f-9313-84d5c11d2d0c   10Gi       RWO            ocs-storagecluster-ceph-rbd   95s
[root@ocpbastion ~]# oc -n dex-base-hqxd5d59 get pods
NAME                                           READY   STATUS    RESTARTS   AGE
cdp-cde-embedded-db-0                          1/1     Running   0          97s
dex-base-configs-manager-6c57c88f69-nwrqk      2/2     Running   0          97s
dex-base-dex-downloads-5c9697dbb4-wm6l4        1/1     Running   0          97s
dex-base-grafana-846c88d599-6xj59              1/1     Running   0          97s
dex-base-hqxd5d59-controller-7bd8d95f4-rgb5p   1/1     Running   0          97s
dex-base-knox-6779786dd5-6hz6t                 1/1     Running   0          97s
dex-base-management-api-797bb8d8f5-nqsdg       1/1     Running   0          97s
fluentd-forwarder-688cb7bc7b-xzbn8             1/1     Running   0          97s


[root@ocpbastion ~]# oc -n dex-app-dzxzpmsh get pvc
NAME                             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
airflow-dags                     Pending                                      nfs            7s
airflow-logs                     Pending                                      nfs            7s
dex-app-dzxzpmsh-livystate-pvc   Pending                                      nfs            7s
dex-app-dzxzpmsh-safari-pvc      Pending                                      nfs            7s
dex-app-dzxzpmsh-storage-pvc     Pending                                      nfs            7s
[root@ocpbastion ~]# oc -n dex-app-dzxzpmsh get pods
NAME                                                  READY   STATUS    RESTARTS   AGE
dex-app-dzxzpmsh-airflow-scheduler-5bd7f969cc-qncpq   0/1     Pending   0          15s
dex-app-dzxzpmsh-airflow-web-76d8b987c9-2rssk         0/1     Pending   0          14s
dex-app-dzxzpmsh-airflowapi-6877c7d55d-2gtl4          0/2     Pending   0          15s
dex-app-dzxzpmsh-api-886b7df76-6gxvf                  0/1     Pending   0          15s
dex-app-dzxzpmsh-livy-5f5b7d95db-nctdk                0/1     Pending   0          15s
dex-app-dzxzpmsh-safari-957b79f49-bs7hq               0/1     Pending   0          15s





ECS:

[root@ecsmaster1 ~]# kubectl -n compute-1656482548-g2jx get pods
NAME                            READY   STATUS    RESTARTS   AGE
das-webapp-0                    1/1     Running   0          5m22s
hiveserver2-0                   1/1     Running   0          5m22s
huebackend-0                    1/1     Running   0          5m22s
huefrontend-589cb7546f-c5d9n    1/1     Running   0          5m22s
query-coordinator-0-0           1/1     Running   0          5m18s
query-coordinator-0-1           1/1     Running   0          5m18s
query-coordinator-0-2           1/1     Running   0          5m18s
query-coordinator-0-3           1/1     Running   0          5m18s
query-executor-0-0              1/1     Running   0          5m18s
query-executor-0-1              1/1     Running   0          5m18s
query-executor-0-2              1/1     Running   0          5m18s
query-executor-0-3              1/1     Running   0          5m18s
standalone-compute-operator-0   1/1     Running   0          5m22s
usage-monitor-97df8f4d4-6qqpf   1/1     Running   0          5m22s


[root@ecsmaster1 ~]# kubectl -n compute-1656482548-g2jx get pvc
NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
query-executor-1656482568-volume-query-executor-0-0   Bound    pvc-656b1bb8-57f8-44dd-8396-a83c4b3f13eb   100Gi      RWO            local-path     2m56s
query-executor-1656482568-volume-query-executor-0-1   Bound    pvc-8716ac21-3dc4-42c6-9a32-1e84c58b8845   100Gi      RWO            local-path     2m56s
query-executor-1656482568-volume-query-executor-0-2   Bound    pvc-7bdff9fe-8135-494a-b0a0-4933d94e4f90   100Gi      RWO            local-path     2m56s
query-executor-1656482568-volume-query-executor-0-3   Bound    pvc-e9318ee8-66fe-4c4a-9a0e-576163d5fa1b   100Gi      RWO            local-path     2m56s

[root@ecsmaster1 ~]# kubectl describe pv pvc-8716ac21-3dc4-42c6-9a32-1e84c58b8845  | grep hostname
    Term 0:        kubernetes.io/hostname in [ecsworker3.cdpkvm.cldr]
[root@ecsmaster1 ~]# kubectl describe pv pvc-e9318ee8-66fe-4c4a-9a0e-576163d5fa1b | grep hostname
    Term 0:        kubernetes.io/hostname in [ecsworker3.cdpkvm.cldr]

[root@ecsmaster1 ~]# kubectl get sc
NAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path           rancher.io/local-path   Delete          WaitForFirstConsumer   false                  38d
longhorn (default)   driver.longhorn.io      Delete          Immediate              true                   38d
longhorn-nfs         nfs.longhorn.io         Delete          Immediate              false                  38d


[root@ecsworker3 ~]# tree /localpath/
/localpath/
|-- local-storage
|   |-- pvc-8716ac21-3dc4-42c6-9a32-1e84c58b8845_compute-1656482548-g2jx_query-executor-1656482568-volume-query-executor-0-1
|   |   `-- llap-2396830618975518217
|   `-- pvc-e9318ee8-66fe-4c4a-9a0e-576163d5fa1b_compute-1656482548-g2jx_query-executor-1656482568-volume-query-executor-0-3
|       `-- llap-5697099011548069627
`-- longhorntmp
    |-- longhorn-disk.cfg
    `-- replicas

7 directories, 1 file




control plane:
[root@ocpbastion ~]# oc adm top pods -n yunikorn
NAME                                             CPU(cores)   MEMORY(bytes)   
yunikorn-admission-controller-58985d458f-2sjvk   2m           19Mi            
yunikorn-scheduler-59886d55b6-h99m2              1601m        239Mi           
[root@ocpbastion ~]# oc adm top pods -n cdp
NAME                                                              CPU(cores)   MEMORY(bytes)   
cdp-release-alert-admin-service-957bfcb58-8wcr4                   2m           88Mi            
cdp-release-classic-clusters-648cc99645-kr5j7                     25m          1058Mi          
cdp-release-cluster-access-manager-69b56fc4db-shvwv               2m           58Mi            
cdp-release-cluster-proxy-1.0.0-7cb6bfbbbd-zwzpr                  21m          940Mi           
cdp-release-cpx-liftie-6568b4669c-tcfwk                           0m           81Mi            
cdp-release-dex-cp-598965f46c-dzg9z                               2m           94Mi            
cdp-release-dmx-55bbfd8cd9-wlqml                                  6m           987Mi           
cdp-release-dps-gateway-1.0-5d58765b8-p5t5m                       9m           114Mi           
cdp-release-dps-gateway-1.0-5d58765b8-scrh2                       7m           110Mi           
cdp-release-dwx-server-76476fc4b5-7nb72                           4m           133Mi           
cdp-release-dwx-ui-865f4998d7-ct5sq                               0m           22Mi            
cdp-release-dwx-ui-865f4998d7-mn7cq                               0m           22Mi            
cdp-release-grafana-54d8b7cf5-64nnm                               6m           193Mi           
cdp-release-logger-alert-receiver-f56f64dc8-4wpzh                 0m           69Mi            
cdp-release-metrics-server-exporter-65f65c54b-2bg7j               1m           74Mi            
cdp-release-monitoring-app-6f6c9b8fb4-dkzdd                       0m           71Mi            
cdp-release-monitoring-metricproxy-b5f85766c-5js7j                0m           71Mi            
cdp-release-monitoring-metricproxy-b5f85766c-6j2dl                0m           68Mi            
cdp-release-monitoring-pvcservice-757bf9bdd4-rmrx4                1m           98Mi            
cdp-release-prometheus-alertmanager-0                             3m           61Mi            
cdp-release-prometheus-alertmanager-1                             1m           59Mi            
cdp-release-prometheus-kube-state-metrics-65c8fd786b-782w2        0m           40Mi            
cdp-release-prometheus-server-c66797d5f-gqlsx                     7m           213Mi           
cdp-release-reloader-676bb9945c-pmjfb                             0m           22Mi            
cdp-release-resource-pool-manager-67dbdfcdd8-6bg9f                6m           51Mi            
cdp-release-thunderhead-cdp-private-authentication-consoletp42g   0m           45Mi            
cdp-release-thunderhead-cdp-private-commonconsole-7ffdf77d4qhnw   0m           44Mi            
cdp-release-thunderhead-cdp-private-environments-console-5ggnpv   0m           44Mi            
cdp-release-thunderhead-compute-api-6b795564fd-s9jj2              0m           779Mi           
cdp-release-thunderhead-consoleauthenticationcdp-8c765dd9725plv   0m           799Mi           
cdp-release-thunderhead-de-api-688c9b6b7-4w5xn                    1m           725Mi           
cdp-release-thunderhead-environment-69b8cd49c5-t878p              10m          1209Mi          
cdp-release-thunderhead-environments2-api-6767bdd568-59r8s        5m           897Mi           
cdp-release-thunderhead-iam-api-7565b47677-njsqt                  0m           816Mi           
cdp-release-thunderhead-iam-console-cdd6dd84f-cp98q               0m           44Mi            
cdp-release-thunderhead-kerberosmgmt-api-68cdd5b56-s7qzc          0m           775Mi           
cdp-release-thunderhead-ml-api-8f488bc95-jhq6k                    1m           765Mi           
cdp-release-thunderhead-resource-management-console-6f8dcfsb7nk   0m           44Mi            
cdp-release-thunderhead-sdx2-api-7476f74476-g7lr7                 1m           747Mi           
cdp-release-thunderhead-servicediscoverysimple-bdf5fb467-8whfw    1m           723Mi           
cdp-release-thunderhead-usermanagement-private-5f494b96dd-vwbwp   14m          958Mi           
dp-mlx-control-plane-app-5ff67bdf77-76t46                         0m           85Mi            
dp-mlx-control-plane-app-health-poller-66fd654d9f-zwvtm           0m           55Mi            
fluentd-aggregator-0                                              10m          3551Mi          
snmp-notifier-7498cc86bc-rk4g6                                    0m           35Mi             
[root@ocpbastion ~]# oc adm top pods -n cdp-env-1-d9aab9ef-monitoring-platform 
NAME                                                        CPU(cores)   MEMORY(bytes)   
monitoring-cm-health-exporter-7944475f99-h7l6b              4m           876Mi           
monitoring-logger-alert-receiver-79fd679b8d-2c7w6           1m           117Mi           
monitoring-metrics-server-exporter-5dc47fbbb9-bq4zb         2m           131Mi           
monitoring-platform-proxy-6777989db7-lxkds                  2m           101Mi           
monitoring-prometheus-alertmanager-0                        3m           112Mi           
monitoring-prometheus-alertmanager-1                        3m           110Mi           
monitoring-prometheus-kube-state-metrics-54b79bd9c8-jpqb7   0m           85Mi            
monitoring-prometheus-pushgateway-99499d6d6-czgcl           1m           89Mi            
monitoring-prometheus-server-6f74fdbc9c-mzvlh               6m           221Mi           
snmp-notifier-765477fc6c-l2rx9                              1m           89Mi   

CML:
[root@ocpbastion ~]# oc adm top pods -n ws1
NAME                                            CPU(cores)   MEMORY(bytes)   
api-786d4c8876-lj6dh                            0m           25Mi            
cron-5df9b4d86d-pwj46                           0m           48Mi            
db-0                                            6m           92Mi            
ds-cdh-client-6bcdb4bfcb-2qjz6                  0m           822Mi           
ds-operator-69d5d4c9db-6pc72                    0m           66Mi            
ds-reconciler-6b894fd4b4-mqmv4                  6m           107Mi           
ds-vfs-b489bcf97-flkgb                          0m           55Mi            
feature-flags-6bfcbbf4c4-t7gvl                  5m           63Mi            
fluentd-forwarder-6f7c84878d-g587g              2m           143Mi           
governance-759d59dd7-pr9qh                      0m           1025Mi          
livelog-0                                       0m           61Mi            
livelog-publisher-8z7vk                         0m           44Mi            
livelog-publisher-h5z9f                         0m           46Mi            
livelog-publisher-slkbf                         0m           43Mi            
model-metrics-6df595b558-xb9rt                  0m           15Mi            
model-metrics-db-0                              4m           23Mi            
model-proxy-6ccb778c9-km86k                     0m           70Mi            
prometheus-postgres-exporter-5956d48756-gkdp9   0m           10Mi            
runtime-manager-64db747ff6-h6m8p                0m           42Mi            
s2i-builder-7b4f878b5d-h5vl9                    0m           340Mi           
s2i-builder-7b4f878b5d-s7rmk                    0m           31Mi            
s2i-builder-7b4f878b5d-svbms                    0m           31Mi            
s2i-client-5f966bc7d9-7jpw7                     0m           69Mi            
s2i-git-server-0                                0m           43Mi            
s2i-queue-0                                     21m          248Mi           
s2i-registry-7fd6658c6c-mzsf7                   1m           63Mi            
s2i-registry-auth-54dfb46556-zc7k4              1m           33Mi            
s2i-server-bd7954c8d-2jbf8                      0m           45Mi            
secret-generator-0                              1m           55Mi            
tcp-ingress-controller-c6c56d7d6-vc72w          0m           36Mi            
usage-reporter-675f6488cc-dmngb                 0m           29Mi            
web-6b9459d445-kjppb                            2m           217Mi           
web-6b9459d445-tkxj7                            0m           242Mi           
web-6b9459d445-zzw7b                            0m           160Mi           
[root@ocpbastion ~]# oc adm top pods -n ws1-user-1
NAME                                           CPU(cores)   MEMORY(bytes)   
churn-model-api-endpoint-3-1-8fc6dc846-4bfr6   2m           261Mi           
ftkstf3074bmexmw                               11m          369Mi 


CDW-impala:
[root@ocpbastion ~]# oc adm top pods -n shared-services
NAME                                    CPU(cores)   MEMORY(bytes)   
log-router-9mdxr                        1416m        324Mi           
log-router-b4jkp                        975m         297Mi           
log-router-bh2tc                        991m         327Mi           
openshift-idling-controller-manager-0   1m           11Mi
[root@ocpbastion ~]# oc adm top pods -n warehouse-1656567061-jjzw
NAME                    CPU(cores)   MEMORY(bytes)   
das-event-processor-0   29m          2850Mi          
metastore-0             39m          3033Mi          
metastore-1             16m          2981Mi 
[root@ocpbastion ~]# oc adm top pods -n impala-1656935869-bvq4
NAME                                 CPU(cores)   MEMORY(bytes)   
catalogd-76b9c5cd86-vgsqp            26m          488Mi           
coordinator-0                        565m         594Mi           
huebackend-0                         121m         216Mi           
impala-autoscaler-855b978c68-s7mfr   15m          14Mi            
impala-executor-000-0                15m          422Mi           
impala-executor-000-1                7m           433Mi           
statestored-8689bbd5db-5pzsx         8m           12Mi 

CDW-hive:
[root@ocpbastion ~]# oc adm top pods -n shared-services
NAME                                    CPU(cores)   MEMORY(bytes)   
log-router-9mdxr                        1416m        324Mi           
log-router-b4jkp                        975m         297Mi           
log-router-bh2tc                        991m         327Mi           
openshift-idling-controller-manager-0   1m           11Mi 
[root@ocpbastion ~]# oc adm top pods -n compute-1656936094-2st6
NAME                             CPU(cores)   MEMORY(bytes)   
das-webapp-0                     18m          424Mi           
hiveserver2-0                    31m          1300Mi          
huebackend-0                     13m          217Mi           
huefrontend-57f8d69bc7-g8lhp     0m           9Mi             
query-coordinator-0-0            41m          703Mi           
query-coordinator-0-1            43m          703Mi           
query-executor-0-0               41m          949Mi           
query-executor-0-1               52m          975Mi           
standalone-compute-operator-0    0m           27Mi            
usage-monitor-6c4fd58bc4-cgtkk   3m           12Mi 

CDE:
[root@ocpbastion ~]# oc -n dex-base-g86mlq9q  adm top pods
NAME                                           CPU(cores)   MEMORY(bytes)   
cdp-cde-embedded-db-0                          8m           505Mi           
dex-base-configs-manager-5f74f57564-64dvh      0m           289Mi           
dex-base-dex-downloads-779d65779c-p2dsr        0m           23Mi            
dex-base-g86mlq9q-controller-c4465b645-ft944   11m          294Mi           
dex-base-grafana-6c65dffc48-2w5lh              0m           24Mi            
dex-base-knox-5d6688b4-cf6wk                   1m           420Mi           
dex-base-management-api-586dfb4c94-d246f       0m           247Mi           
fluentd-forwarder-757d65d65c-kghfm             0m           91Mi 

[root@ocpbastion ~]# oc -n dex-app-p5q6xqdq get pods
NAME                                                 READY   STATUS    RESTARTS   AGE
dex-app-p5q6xqdq-airflow-scheduler-b78794fc8-v5wt8   0/1     Pending   0          61s
dex-app-p5q6xqdq-airflow-web-57f878b97f-vg74v        0/1     Pending   0          61s
dex-app-p5q6xqdq-airflowapi-64d4bf6767-b7pt2         0/2     Pending   0          61s
dex-app-p5q6xqdq-api-7bccbf4877-z9vj5                0/1     Pending   0          61s
dex-app-p5q6xqdq-livy-7b69c9d7f9-s4d44               0/1     Pending   0          61s
dex-app-p5q6xqdq-safari-5745ddd96d-rjblh             0/1     Pending   0          61s



CDW (not low resource mode)

[root@ocpbastion ~]# oc -n impala-1657000326-zvn5 describe pvc scratch-cache-volume-coordinator-0 
Name:          scratch-cache-volume-coordinator-0
Namespace:     impala-1657000326-zvn5
StorageClass:  cdw-disk
Status:        Pending
Volume:        
Labels:        app=coordinator
Annotations:   <none>
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      
Access Modes:  
VolumeMode:    Filesystem
Used By:       coordinator-0
Events:
  Type    Reason                Age               From                         Message
  ----    ------                ----              ----                         -------
  Normal  WaitForFirstConsumer  60s               persistentvolume-controller  waiting for first consumer to be created before binding
  Normal  WaitForPodScheduled   1s (x4 over 46s)  persistentvolume-controller  waiting for pod coordinator-0 to be scheduled
[root@ocpbastion ~]# oc -n impala-1657000326-zvn5 get pvc scratch-cache-volume-coordinator-0 -o yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  creationTimestamp: "2022-07-05T05:52:25Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app: coordinator
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .: {}
          f:app: {}
      f:spec:
        f:accessModes: {}
        f:resources:
          f:requests:
            .: {}
            f:storage: {}
        f:storageClassName: {}
        f:volumeMode: {}
      f:status:
        f:phase: {}
    manager: kube-controller-manager
    operation: Update
    time: "2022-07-05T05:52:25Z"
  name: scratch-cache-volume-coordinator-0
  namespace: impala-1657000326-zvn5
  resourceVersion: "5586288"
  selfLink: /api/v1/namespaces/impala-1657000326-zvn5/persistentvolumeclaims/scratch-cache-volume-coordinator-0
  uid: f385a97c-dfd0-44f7-ade7-265fdc8518c8
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 600Gi
  storageClassName: cdw-disk
  volumeMode: Filesystem
status:
  phase: Pending

[root@ocpbastion ~]# oc -n impala-1657000326-zvn5 get pvc scratch-cache-volume-impala-executor-000-0 -o yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  creationTimestamp: "2022-07-05T05:52:25Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app: impala-executor
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .: {}
          f:app: {}
      f:spec:
        f:accessModes: {}
        f:resources:
          f:requests:
            .: {}
            f:storage: {}
        f:storageClassName: {}
        f:volumeMode: {}
      f:status:
        f:phase: {}
    manager: kube-controller-manager
    operation: Update
    time: "2022-07-05T05:52:25Z"
  name: scratch-cache-volume-impala-executor-000-0
  namespace: impala-1657000326-zvn5
  resourceVersion: "5586245"
  selfLink: /api/v1/namespaces/impala-1657000326-zvn5/persistentvolumeclaims/scratch-cache-volume-impala-executor-000-0
  uid: 1926bb4e-ac1c-4357-9127-bf68eebbd640
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 600Gi
  storageClassName: cdw-disk
  volumeMode: Filesystem
status:
  phase: Pending

impala:
# oc -n impala-1657000326-zvn5 get pod impala-executor-000-0  -o yaml
    resources:
      limits:
        memory: 116736M
      requests:
        cpu: "14"
        memory: 116736M

# oc -n impala-1657000326-zvn5 get pod coordinator-0  -o yaml
      limits:
        memory: 102400M
      requests:
        cpu: "8"
        memory: 102400M        
        
hive:
# oc -n compute-1657001663-wz6k get pod query-coordinator-0-0  -o yaml
    resources:
      limits:
        cpu: "2"
        memory: 4096M
      requests:
        cpu: "2"
        memory: 4096M

# oc -n compute-1657001663-wz6k get pod query-executor-0-0 -o yaml
    resources:
      limits:
        cpu: "12"
        memory: 116736M
      requests:
        cpu: "12"
        memory: 116736M



cp -r /sdx-templates/* /sdx-templates-shared
echo "hive/dwx-env-6hcbzr" > /sdx-templates-shared/hive.krb5template
cp /tmp/hive/conf/* /etc/hive/conf/

/usr/local/bin/config-merger /tmp/hive/conf/hive-site.xml /mnt/config/current/hive-conf/hive-site.xml /etc/hive/conf/hive-site.xml -wf2 hive.metastore.kerberos.principal -set hive.server2.authentication.kerberos.principal='${SERVICE_PRINCIPAL}' -set hive.server2.authentication.kerberos.keytab=/mnt/config/current/hive -set hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.ZooKeeperTokenStore -set hive.metastore.sasl.enabled=true

/usr/local/bin/config-merger /tmp/hadoop/conf/core-site.xml /mnt/config/current/hive-conf/core-site.xml /etc/hadoop/conf/core-site.xml
cp /mnt/config/current/hive-conf/yarn-site.xml /etc/hadoop/conf/yarn-site.xml
cp /mnt/config/current/hive-conf/hdfs-site.xml /etc/hadoop/conf/hdfs-site.xml
export OZONE_SITE_LOCATION=/mnt/config/current/hive-conf/ozone-site.xml; if [ -f "$OZONE_SITE_LOCATION" ]; then cp $OZONE_SITE_LOCATION /etc/hadoop/conf/ozone-site.xml; else echo "File does not exist: $OZONE_SITE_LOCATION"; fi;
 cp /mnt/config/current/krb5.conf /etc/krb5.conf && klist -kt /mnt/config/current/hive | sed -n -e '$p'|awk '{ print $NF }' | sed 's/^/export SERVICE_PRINCIPAL=/' > /mnt/config/current/principal.env
 
 echo "export RANGER_URL=\"$(awk '/ranger.plugin.hive.policy.rest.url/{getline; print}' /mnt/config/current/ranger-hive-security.xml | sed -E 's/^.*<value>(.*)<\/value>$/\1/g' | sed 's/\/$//' )\"" >> /mnt/config/current/principal.env
 
 echo "export RANGER_CLUSTER_NAME=\"$(awk '/ranger.plugin.hive.access.cluster.name/{getline; print}' /mnt/config/current/ranger-hive-security.xml | cut -d '>' -f 2 | cut -f 1 -d'<')\"" >> /mnt/config/current/principal.env
 
 source /mnt/config/current/principal.env; find /etc/hive/conf/ -type f | while read file; do cat $file | envsubst '$SERVICE_PRINCIPAL' | envsubst '$HADOOP_USER_NAME' | envsubst '$WAREHOUSE_DIR' > $file.new; mv $file.new $file; done
 
 
  while ! nc -zv -w 2 metastore-service 9083; do echo "waiting for metastore-service to connect on 9083.."; sleep 5; done;
  
   /sys-entrypoint.sh


CREATE EXTERNAL TABLE IF NOT EXISTS testdb.students(
  student_ID INT, FirstName STRING, LastName STRING,    
  year STRING, Major STRING)
  COMMENT 'Student Names'
  ROW FORMAT DELIMITED
  FIELDS TERMINATED BY ','
  STORED AS TEXTFILE
  LOCATION 'hdfs://test/hivedir'
  
CREATE EXTERNAL TABLE db1.tmp(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  COMMENT 'Residents Details'
  ROW FORMAT DELIMITED
  FIELDS TERMINATED BY ','
  STORED AS TEXTFILE
  LOCATION 'hdfs:/tmp/residents'

CREATE TABLE db1.avro(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS avro
  
INSERT INTO TABLE avro SELECT * FROM tmp

CREATE TABLE db1.parquet(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS parquet

CML:
# # Word counts
# 
# This example shows how to count the occurrences of each word in a text file.

from __future__ import print_function
import sys, re
from operator import add
from pyspark.sql import SparkSession

spark = SparkSession\
    .builder\
    .appName("PythonWordCount")\
    .getOrCreate()

# Add the data file to hdfs.
!hdfs dfs -put resources/cgroup-v2.txt /tmp

lines = spark.read.text("/tmp/cgroup-v2.txt").rdd.map(lambda r: r[0])
counts = lines.flatMap(lambda x: x.split(' ')) \
              .map(lambda x: (x, 1)) \
              .reduceByKey(add) \
              .sortBy(lambda x: x[1], False)
output = counts.collect()
for (word, count) in output:
    print("%s: %i" % (word, count))

spark.stop()





[root@ocpbastion ~]# oc get networkpolicy
NAME                           POD-SELECTOR   AGE
allow-from-openshift-ingress   <none>         24h
allow-from-workspace2          <none>         24h
allow-from-workspace2-user-1   <none>         24h
(reverse-i-search)`-0': oc exec -ti huebackend^C  -- /bin/sh
[root@ocpbastion ~]# oc get networkpolicy allow-from-openshift-ingress -o yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: "2022-07-07T08:34:28Z"
  generation: 1
  managedFields:
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:ingress: {}
        f:policyTypes: {}
    manager: operator
    operation: Update
    time: "2022-07-07T08:34:28Z"
  name: allow-from-openshift-ingress
  namespace: workspace2-user-1
  resourceVersion: "6934442"
  selfLink: /apis/networking.k8s.io/v1/namespaces/workspace2-user-1/networkpolicies/allow-from-openshift-ingress
  uid: 565d34d6-f2e3-4cfd-9116-25da69927804
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
[root@ocpbastion ~]# oc get networkpolicy allow-from-workspace2 -o yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: "2022-07-07T08:34:28Z"
  generation: 1
  managedFields:
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:ingress: {}
        f:policyTypes: {}
    manager: operator
    operation: Update
    time: "2022-07-07T08:34:28Z"
  name: allow-from-workspace2
  namespace: workspace2-user-1
  resourceVersion: "6934434"
  selfLink: /apis/networking.k8s.io/v1/namespaces/workspace2-user-1/networkpolicies/allow-from-workspace2
  uid: 10fe39ef-f044-4926-bd47-e629781703d4
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: workspace2
  podSelector: {}
  policyTypes:
  - Ingress
[root@ocpbastion ~]# oc get networkpolicy allow-from-workspace2-user-1 -o yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: "2022-07-07T08:34:28Z"
  generation: 1
  managedFields:
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:ingress: {}
        f:policyTypes: {}
    manager: operator
    operation: Update
    time: "2022-07-07T08:34:28Z"
  name: allow-from-workspace2-user-1
  namespace: workspace2-user-1
  resourceVersion: "6934428"
  selfLink: /apis/networking.k8s.io/v1/namespaces/workspace2-user-1/networkpolicies/allow-from-workspace2-user-1
  uid: fa1dac31-8779-420b-b1a7-817a62f0c125
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: workspace2-user-1
  podSelector: {}
  policyTypes:
  - Ingress
  
  
  