ocpbastion:

openssl crl2pkcs7 -nocrl -certfile /etc/pki/tls/certs/ca-bundle.crt | openssl pkcs7 -print_certs | grep subject > pkiorig

openssl crl2pkcs7 -nocrl -certfile /etc/pki/tls/certs/ca-bundle.crt | openssl pkcs7 -print_certs | grep subject | wc -l
127


[root@ocpbastion ~]# oc get clusterversions.config.openshift.io
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.7.53    True        False         12m     Cluster version is 4.7.53

[root@ocpbastion ~]# oc get nodes
NAME                        STATUS   ROLES           AGE   VERSION
master01.ocp4.cdpkvm.cldr   Ready    master,worker   31m   v1.20.15+98b2293
master02.ocp4.cdpkvm.cldr   Ready    master,worker   31m   v1.20.15+98b2293
master03.ocp4.cdpkvm.cldr   Ready    master,worker   31m   v1.20.15+98b2293

[root@ocpbastion ~]# oc get co
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             4.7.53    True        False         False      14m
baremetal                                  4.7.53    True        False         False      28m
cloud-credential                           4.7.53    True        False         False      39m
cluster-autoscaler                         4.7.53    True        False         False      28m
config-operator                            4.7.53    True        False         False      30m
console                                    4.7.53    True        False         False      19m
csi-snapshot-controller                    4.7.53    True        False         False      24m
dns                                        4.7.53    True        False         False      28m
etcd                                       4.7.53    True        False         False      28m
image-registry                             4.7.53    True        False         False      24m
ingress                                    4.7.53    True        False         False      26m
insights                                   4.7.53    True        False         False      23m
kube-apiserver                             4.7.53    True        False         False      27m
kube-controller-manager                    4.7.53    True        False         False      27m
kube-scheduler                             4.7.53    True        False         False      28m
kube-storage-version-migrator              4.7.53    True        False         False      29m
machine-api                                4.7.53    True        False         False      29m
machine-approver                           4.7.53    True        False         False      29m
machine-config                             4.7.53    True        False         False      29m
marketplace                                4.7.53    True        False         False      29m
monitoring                                 4.7.53    True        False         False      23m
network                                    4.7.53    True        False         False      29m
node-tuning                                4.7.53    True        False         False      29m
openshift-apiserver                        4.7.53    True        False         False      24m
openshift-controller-manager               4.7.53    True        False         False      27m
openshift-samples                          4.7.53    True        False         False      23m
operator-lifecycle-manager                 4.7.53    True        False         False      29m
operator-lifecycle-manager-catalog         4.7.53    True        False         False      29m
operator-lifecycle-manager-packageserver   4.7.53    True        False         False      24m
service-ca                                 4.7.53    True        False         False      30m
storage                                    4.7.53    True        False         False      30m


[root@ocpbastion ~]# oc get pods -n openshift-ingress
NAME                              READY   STATUS    RESTARTS   AGE
router-default-5b7459c989-fcf4w   1/1     Running   0          91m
router-default-5b7459c989-jvhpr   1/1     Running   0          91m


[root@ocpbastion ~]# oc describe pod router-default-5b7459c989-fcf4w -n openshift-ingress | grep -i "Node:"
Node:                 master01.ocp4.cdpkvm.cldr/10.15.4.182
[root@ocpbastion ~]# oc describe pod router-default-5b7459c989-jvhpr -n openshift-ingress | grep -i "Node:"
Node:                 master02.ocp4.cdpkvm.cldr/10.15.4.183


[root@ocpbastion ~]# oc debug node/master01.ocp4.cdpkvm.cldr
Starting pod/master01ocp4cdpkvmcldr-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.15.4.182
If you don't see a command prompt, try pressing enter.
sh-4.4# chroot /host
sh-4.4# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0  100G  0 disk 
|-vda1 252:1    0    1M  0 part 
|-vda2 252:2    0  127M  0 part 
|-vda3 252:3    0  384M  0 part /boot
`-vda4 252:4    0 99.5G  0 part /sysroot
vdb    252:16   0  250G  0 disk 
vdc    252:32   0  400G  0 disk 


[root@ocpbastion ~]# oc get nodes -l cluster.ocs.openshift.io/openshift-storage=
NAME                        STATUS   ROLES           AGE     VERSION
master01.ocp4.cdpkvm.cldr   Ready    master,worker   4h31m   v1.20.15+98b2293
master02.ocp4.cdpkvm.cldr   Ready    master,worker   4h31m   v1.20.15+98b2293
master03.ocp4.cdpkvm.cldr   Ready    master,worker   4h31m   v1.20.15+98b2293





[root@ocpbastion ~]# oc get sc
NAME                                    PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
cdw-disk                                kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  22m
ocs-storagecluster-ceph-rbd (default)   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   40m
ocs-storagecluster-ceph-rgw             openshift-storage.ceph.rook.io/bucket   Delete          Immediate              false                  40m
ocs-storagecluster-cephfs               openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   40m
openshift-storage.noobaa.io             openshift-storage.noobaa.io/obc         Delete          Immediate              false                  36m
vdb                                     kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  42m




[root@ocpbastion ~]# oc get pods -n cdp | head
NAME                                                              READY   STATUS                  RESTARTS   AGE
cdp-release-alert-admin-service-957bfcb58-hz8g7                   0/2     ImagePullBackOff        0          77s
cdp-release-classic-clusters-7df4fb5ccb-94bsl                     0/3     Init:ImagePullBackOff   0          71s
cdp-release-cluster-access-manager-57447b4895-bnfc7               0/2     ImagePullBackOff        0          76s
cdp-release-cluster-proxy-1.0.0-67b59bd449-nz8pb                  0/2     ErrImagePull            0          72s
cdp-release-cpx-liftie-7dccf746b7-85djb                           0/2     ImagePullBackOff        0          82s
cdp-release-dex-cp-74788c5f79-t6cqt                               0/2     ImagePullBackOff        0          79s
cdp-release-dmx-55bbfd8cd9-skkg6                                  0/3     ImagePullBackOff        0          71s
cdp-release-dps-gateway-1.0-5d58765b8-6p4ff                       0/3     ImagePullBackOff        0          87s
cdp-release-dps-gateway-1.0-5d58765b8-jbfqj                       0/3     ImagePullBackOff        0          87s
[root@ocpbastion ~]# oc describe pod cdp-release-alert-admin-service-957bfcb58-hz8g7 -n cdp | tail
  Normal   Pulling         62s (x2 over 78s)  kubelet            Pulling image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera_thirdparty/fluent-bit:v1.4.6-3896242"
  Warning  Failed          62s (x2 over 78s)  kubelet            Failed to pull image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera/monitoring-app:1.3.5-b30": rpc error: code = Unknown desc = error pinging docker registry nexus.cdpkvm.cldr:9999: Get "https://nexus.cdpkvm.cldr:9999/v2/": x509: certificate signed by unknown authority
  Warning  Failed          62s (x2 over 78s)  kubelet            Error: ErrImagePull
  Normal   Pulling         62s (x2 over 78s)  kubelet            Pulling image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera/monitoring-app:1.3.5-b30"
  Warning  Failed          62s (x2 over 78s)  kubelet            Failed to pull image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera_thirdparty/fluent-bit:v1.4.6-3896242": rpc error: code = Unknown desc = error pinging docker registry nexus.cdpkvm.cldr:9999: Get "https://nexus.cdpkvm.cldr:9999/v2/": x509: certificate signed by unknown authority
  Warning  Failed          62s (x2 over 78s)  kubelet            Error: ErrImagePull
  Normal   BackOff         49s (x3 over 78s)  kubelet            Back-off pulling image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera/monitoring-app:1.3.5-b30"
  Warning  Failed          49s (x3 over 78s)  kubelet            Error: ImagePullBackOff
  Normal   BackOff         49s (x3 over 78s)  kubelet            Back-off pulling image "nexus.cdpkvm.cldr:9999/cdppvc/cloudera_thirdparty/fluent-bit:v1.4.6-3896242"
  Warning  Failed          49s (x3 over 78s)  kubelet            Error: ImagePullBackOff
  
  
https://docs.openshift.com/container-platform/4.7/cicd/builds/setting-up-trusted-ca.html


[root@ocpbastion ~]# oc create configmap registry-cas -n openshift-config --from-file=nexus.cdpkvm.cldr..9999=/root/nexus.crt 
configmap/registry-cas created
[root@ocpbastion ~]# oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-cas"}}}' --type=merge
image.config.openshift.io/cluster patched


[root@ocpbastion ~]# oc get pvc
NAME                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
cdp-release-prometheus-server                          Bound    pvc-63b9d337-d7c1-4051-b4c5-e46949affd0d   10Gi       RWO            ocs-storagecluster-ceph-rbd   23m
logs                                                   Bound    pvc-2c623b62-e90b-40b2-92c5-e4343528885a   20Gi       RWO            ocs-storagecluster-ceph-rbd   23m
storage-volume-cdp-release-prometheus-alertmanager-0   Bound    pvc-e3e0ac29-70b1-445e-80b3-b7a92a8f1923   2Gi        RWO            ocs-storagecluster-ceph-rbd   23m
storage-volume-cdp-release-prometheus-alertmanager-1   Bound    pvc-93b032f5-2115-4c92-b77c-940458d39412   2Gi        RWO            ocs-storagecluster-ceph-rbd   10m


CDW:

[root@ocpbastion ~]# oc -n shared-services get pods
NAME                                    READY   STATUS    RESTARTS   AGE
log-router-g787n                        2/2     Running   0          55s
log-router-q8j6c                        2/2     Running   0          55s
log-router-xfrbt                        2/2     Running   0          55s
openshift-idling-controller-manager-0   1/1     Running   0          54s




https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.6/html/deploying_openshift_container_storage_using_bare_metal_infrastructure/deploy-using-local-storage-devices-bm#installing-local-storage-operator_rhocs

[root@ocpbastion ~]# oc -n warehouse-1656305490-mgc9 get pvc
No resources found in warehouse-1656305490-mgc9 namespace.
[root@ocpbastion ~]# oc -n warehouse-1656305490-mgc9 get pods
NAME                    READY   STATUS    RESTARTS   AGE
das-event-processor-0   1/1     Running   0          8m23s
metastore-0             1/1     Running   0          8m23s
metastore-1             1/1     Running   0          5m54s



[root@ocpbastion ~]# oc get sc
NAME                                    PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
cdw-disk                                kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  2m9s
ocs-storagecluster-ceph-rbd (default)   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   24h
ocs-storagecluster-ceph-rgw             openshift-storage.ceph.rook.io/bucket   Delete          Immediate              false                  24h
ocs-storagecluster-cephfs               openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   24h
openshift-storage.noobaa.io             openshift-storage.noobaa.io/obc         Delete          Immediate              false                  24h
vdb                                     kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  24h

[root@ocpbastion ~]# oc get pv | head -1; oc get pv | grep local
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                                        STORAGECLASS                  REASON   AGE
local-pv-12e7ab5                           400Gi      RWO            Delete           Available                                                                                                cdw-disk                               2m33s
local-pv-17bf8fbc                          300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-2slfmj                                            vdb                                    24h
local-pv-33a6a00a                          400Gi      RWO            Delete           Available                                                                                                cdw-disk                               2m34s
local-pv-62cdcca                           300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-06dblw                                            vdb                                    24h
local-pv-abb1e063                          400Gi      RWO            Delete           Available                                                                                                cdw-disk                               2m34s
local-pv-aca516bf                          300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-1t74g8                                            vdb                                    24h


[root@ocpbastion ~]# oc describe pv local-pv-12e7ab5
Name:              local-pv-12e7ab5
Labels:            storage.openshift.com/local-volume-owner-name=cdw-localdisk-each-worker
                   storage.openshift.com/local-volume-owner-namespace=openshift-local-storage
Annotations:       pv.kubernetes.io/provisioned-by: local-volume-provisioner-master01.ocp4.cdpkvm.cldr-4603dd1e-328f-46b2-b086-48f98ec5d9e7
Finalizers:        [kubernetes.io/pv-protection]
StorageClass:      cdw-disk
Status:            Available
Claim:             
Reclaim Policy:    Delete
Access Modes:      RWO
VolumeMode:        Filesystem
Capacity:          400Gi
Node Affinity:     
  Required Terms:  
    Term 0:        kubernetes.io/hostname in [master01.ocp4.cdpkvm.cldr]
Message:           
Source:
    Type:  LocalVolume (a persistent volume backed by local storage on a node)
    Path:  /mnt/local-storage/cdw-disk/vdc
Events:    <none>




provision hive (1 executor):

[root@ocpbastion ~]# oc -n compute-1656330332-ph2j get pods
NAME                             READY   STATUS    RESTARTS   AGE
das-webapp-0                     1/1     Running   0          2m43s
hiveserver2-0                    1/1     Running   0          2m43s
huebackend-0                     1/1     Running   0          2m43s
huefrontend-6bcbb8fdfb-8kb7s     1/1     Running   0          2m43s
query-coordinator-0-0            1/1     Running   0          2m32s
query-executor-0-0               1/1     Running   0          2m32s
standalone-compute-operator-0    1/1     Running   0          2m43s
usage-monitor-564fdcdbcc-mbgmc   1/1     Running   0          2m43s

[root@ocpbastion ~]# oc -n compute-1656330332-ph2j get pvc
NAME                                                  STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
query-executor-1656330358-volume-query-executor-0-0   Bound    local-pv-abb1e063   400Gi      RWO            cdw-disk       2m34s


[root@ocpbastion ~]# oc -n compute-1656330332-ph2j describe pod query-executor-0-0 | grep Node:
Node:         master03.ocp4.cdpkvm.cldr/10.15.4.184

[root@ocpbastion ~]# oc describe pv local-pv-abb1e063 | grep master
                   pv.kubernetes.io/provisioned-by: local-volume-provisioner-master03.ocp4.cdpkvm.cldr-43054c69-a851-47c4-a3c3-b7d2b74d8a2e
    Term 0:        kubernetes.io/hostname in [master03.ocp4.cdpkvm.cldr]

[root@ocpbastion ~]# oc describe pv local-pv-abb1e063 | grep hostname
    Term 0:        kubernetes.io/hostname in [master03.ocp4.cdpkvm.cldr]
    
    
provision impala (1 executor):

[root@ocpbastion ~]# oc -n impala-1656330683-rm4v get pods
NAME                                 READY   STATUS    RESTARTS   AGE
catalogd-74869dbf89-ldrmh            1/1     Running   0          2m2s
coordinator-0                        4/4     Running   0          2m2s
coordinator-1                        3/4     Running   0          80s
huebackend-0                         2/2     Running   0          2m2s
huefrontend-74c45775c-zz2lv          1/1     Running   0          2m2s
impala-autoscaler-5b484b9fb7-znsxw   1/1     Running   0          2m1s
impala-executor-000-0                1/1     Running   0          2m2s
statestored-694d8b5fbb-529tp         1/1     Running   0          2m2s
usage-monitor-755869884-ww6cn        1/1     Running   0          2m2s

[root@ocpbastion ~]# oc -n impala-1656330683-rm4v get pvc
NAME                                         STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
scratch-cache-volume-coordinator-0           Bound    local-pv-abb1e063   400Gi      RWO            cdw-disk       9s
scratch-cache-volume-impala-executor-000-0   Bound    local-pv-33a6a00a   400Gi      RWO            cdw-disk       9s
[root@ocpbastion ~]# oc ^C
[root@ocpbastion ~]# oc -n impala-1656330683-rm4v describe pod coordinator-0 | grep Node:
Node:         master03.ocp4.cdpkvm.cldr/10.15.4.184
[root@ocpbastion ~]# oc describe pv local-pv-abb1e063 | grep hostname
    Term 0:        kubernetes.io/hostname in [master03.ocp4.cdpkvm.cldr]
[root@ocpbastion ~]# oc -n impala-1656330683-rm4v describe pod impala-executor-000-0 | grep Node:
Node:         master02.ocp4.cdpkvm.cldr/10.15.4.183
[root@ocpbastion ~]# oc describe pv local-pv-33a6a00a | grep hostname
    Term 0:        kubernetes.io/hostname in [master02.ocp4.cdpkvm.cldr]
[root@ocpbastion ~]# oc get sc | head -1 ; oc get sc | grep cdw-disk
NAME                                    PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
cdw-disk                                kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  53s
[root@ocpbastion ~]# ssh core@master03.ocp4.cdpkvm.cldr
Red Hat Enterprise Linux CoreOS 47.84.202206080457-0
  Part of OpenShift 4.7, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.7/architecture/architecture-rhcos.html

---
Last login: Mon Jun 27 11:27:34 2022 from 10.15.4.189
[core@master03 ~]$ ll /mnt/local-storage/
total 0
drwxr-xr-x. 2 root root 17 Jun 27 08:41 cdw-disk
drwxr-xr-x. 2 root root 17 Jun 26 15:18 vdb
[core@master03 ~]$ ll /mnt/local-storage/cdw-disk
total 0
lrwxrwxrwx. 1 root root 8 Jun 27 08:41 vdc -> /dev/vdc                                             cdw-disk                               11m


[root@ocpbastion ~]# oc get sc | head -1 ; oc get sc | grep cdw-disk
NAME                                    PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
cdw-disk                                kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  53s


Add new disk in cdw-disk in local storage operator


[root@ocpbastion ~]# ssh core@master02.ocp4.cdpkvm.cldr
Red Hat Enterprise Linux CoreOS 47.84.202206080457-0
  Part of OpenShift 4.7, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.7/architecture/architecture-rhcos.html

---
Last login: Mon Jun 27 11:27:22 2022 from 10.15.4.189
[core@master02 ~]$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0    7:0    0  300G  0 loop 
rbd0   251:0    0    2G  0 disk /var/lib/kubelet/pods/0bd4e204-3a6f-457f-8b05-68b4ae4b59a5/volumes/kubernetes.io~csi/pvc-8dfd9e8b-4ce5-4b9d-aa35-04ef516ca124/
rbd1   251:16   0    2G  0 disk /var/lib/kubelet/pods/bc1f562c-de29-48d8-92d5-55e73db4cbf3/volumes/kubernetes.io~csi/pvc-1aa5ade9-5ace-4058-a074-957abf8aaaaa/
rbd2   251:32   0   20G  0 disk /var/lib/kubelet/pods/e04b26fe-229b-4fec-b056-4cd67c9950a1/volumes/kubernetes.io~csi/pvc-cfa677f6-5fd3-4d96-a5b5-dda4162a36f5/
vda    252:0    0  100G  0 disk 
|-vda1 252:1    0    1M  0 part 
|-vda2 252:2    0  127M  0 part 
|-vda3 252:3    0  384M  0 part /boot
`-vda4 252:4    0 99.5G  0 part /sysroot
vdb    252:16   0  300G  0 disk 
vdc    252:32   0  400G  0 disk 
vdd    252:48   0  200G  0 disk 
[core@master02 ~]$ ll /mnt/local-storage/
total 0
drwxr-xr-x. 2 root root 17 Jun 27 08:41 cdw-disk
drwxr-xr-x. 2 root root 17 Jun 26 15:17 vdb
[core@master02 ~]$ ll /mnt/local-storage/cdw-disk/
total 0
lrwxrwxrwx. 1 root root 8 Jun 27 08:41 vdc -> /dev/vdc


[core@master02 ~]$ ll /mnt/local-storage/cdw-disk/
total 0
lrwxrwxrwx. 1 root root 8 Jun 27 08:41 vdc -> /dev/vdc
lrwxrwxrwx. 1 root root 8 Jun 27 12:00 vdd -> /dev/vdd

[root@ocpbastion ~]# oc get pv | head -1; oc get pv | grep local
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                                        STORAGECLASS                  REASON   AGE
local-pv-12e7ab5                           400Gi      RWO            Delete           Available                                                                                                cdw-disk                               5m59s
local-pv-17bf8fbc                          300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-2slfmj                                            vdb                                    25h
local-pv-33a6a00a                          400Gi      RWO            Delete           Available                                                                                                cdw-disk                               6m40s
local-pv-62cdcca                           300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-06dblw                                            vdb                                    25h
local-pv-abb1e063                          400Gi      RWO            Delete           Available                                                                                                cdw-disk                               6m10s
local-pv-aca516bf                          300Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-vdb-0-data-1t74g8                                            vdb                                    25h
local-pv-c8fe6eea                          200Gi      RWO            Delete           Available                                                                                                cdw-disk                               85s
local-pv-d30bd8ac                          200Gi      RWO            Delete           Available                                                                                                cdw-disk                               77s
local-pv-dc533915                          200Gi      RWO            Delete           Available                                                                                                cdw-disk                               66s


impala 4 executor:

[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 get pods
NAME                                READY   STATUS    RESTARTS   AGE
catalogd-567f684bcb-4dxph           1/1     Running   0          80s
coordinator-0                       4/4     Running   0          80s
huebackend-0                        2/2     Running   0          80s
huefrontend-7c765bf44d-47dkq        1/1     Running   0          80s
impala-autoscaler-6c7564d7f-lpndc   1/1     Running   0          80s
impala-executor-000-0               1/1     Running   0          80s
impala-executor-000-1               1/1     Running   0          80s
impala-executor-000-2               1/1     Running   0          80s
impala-executor-000-3               1/1     Running   0          80s
statestored-f9d6b4bb7-hhk65         1/1     Running   0          80s
usage-monitor-8677878c4b-q5hx8      1/1     Running   0          80s

[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 get pvc
NAME                                         STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
scratch-cache-volume-coordinator-0           Bound    local-pv-abb1e063   400Gi      RWO            cdw-disk       103s
scratch-cache-volume-impala-executor-000-0   Bound    local-pv-d30bd8ac   200Gi      RWO            cdw-disk       103s
scratch-cache-volume-impala-executor-000-1   Bound    local-pv-dc533915   200Gi      RWO            cdw-disk       103s
scratch-cache-volume-impala-executor-000-2   Bound    local-pv-c8fe6eea   200Gi      RWO            cdw-disk       103s
scratch-cache-volume-impala-executor-000-3   Bound    local-pv-33a6a00a   400Gi      RWO            cdw-disk       103s


[root@ocpbastion ~]# oc describe pv local-pv-33a6a00a | grep hostname
    Term 0:        kubernetes.io/hostname in [master02.ocp4.cdpkvm.cldr]

[root@ocpbastion ~]# oc describe pv local-pv-dc533915 | grep hostname
    Term 0:        kubernetes.io/hostname in [master02.ocp4.cdpkvm.cldr]
    
[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 describe pod impala-executor-000-1 | grep Node:
Node:         master02.ocp4.cdpkvm.cldr/10.15.4.183

[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 describe pod impala-executor-000-3 | grep Node:
Node:         master02.ocp4.cdpkvm.cldr/10.15.4.183


[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 exec -ti impala-executor-000-1 -- /bin/sh
sh-4.2$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
rbd0   251:0    0    2G  0 disk 
vdd    252:48   0  200G  0 disk /opt/impala/scratch
vdb    252:16   0  300G  0 disk 
rbd1   251:16   0    2G  0 disk 
loop0    7:0    0  300G  0 loop 
vdc    252:32   0  400G  0 disk 
vda    252:0    0  100G  0 disk 
|-vda4 252:4    0 99.5G  0 part /opt/impala/logs/minidumps
|-vda2 252:2    0  127M  0 part 
|-vda3 252:3    0  384M  0 part 
`-vda1 252:1    0    1M  0 part 
rbd2   251:32   0   20G  0 disk 

sh-4.2$ ls -l /opt/impala/scratch/
total 0
-rw-r--r--. 1 hive 1000800000 0 Jun 27 12:35 impala-cache-file-524e9102ab341ef1:1917da93c3905ab4
drwxr-sr-x. 2 hive 1000800000 6 Jun 27 12:35 impala-scratch


[root@ocpbastion ~]# oc -n impala-1656333286-nrm8 exec -ti impala-executor-000-3 -- /bin/sh
sh-4.2$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
rbd0   251:0    0    2G  0 disk 
vdd    252:48   0  200G  0 disk 
vdb    252:16   0  300G  0 disk 
rbd1   251:16   0    2G  0 disk 
loop0    7:0    0  300G  0 loop 
vdc    252:32   0  400G  0 disk /opt/impala/scratch
vda    252:0    0  100G  0 disk 
|-vda4 252:4    0 99.5G  0 part /opt/impala/logs/minidumps
|-vda2 252:2    0  127M  0 part 
|-vda3 252:3    0  384M  0 part 
`-vda1 252:1    0    1M  0 part 
rbd2   251:32   0   20G  0 disk 
sh-4.2$ ls -l /opt/impala/scratch
total 0
-rw-r--r--. 1 hive 1000800000 0 Jun 27 12:46 impala-cache-file-154c4655952ade0d:c400e0289decbc93
drwxr-sr-x. 2 hive 1000800000 6 Jun 27 12:46 impala-scratch



ocpcml:

[root@ocpbastion ~]# oc get pods
NAME                                             READY   STATUS      RESTARTS   AGE
api-75f7d749f-8r72d                              1/1     Running     0          6m54s
cron-7bbfc48b5b-svxz2                            2/2     Running     0          6m54s
db-0                                             2/2     Running     0          6m54s
db-migrate-2.0.28-b66-9nj26                      0/1     Completed   0          6m44s
ds-cdh-client-b8dd5d99-f5c7f                     3/3     Running     0          6m54s
ds-operator-6f4c4c8957-wjjtk                     2/2     Running     1          6m54s
ds-reconciler-7fccf5cbd9-nfcjv                   2/2     Running     0          6m54s
ds-vfs-689c467d96-tttxv                          2/2     Running     0          6m54s
feature-flags-69447f8575-2jjzz                   2/2     Running     0          6m54s
fluentd-forwarder-5d855989fc-dxxvl               1/1     Running     0          6m54s
hadoop-cli-7.2.10-hf1-3z0poka-bwjqt              0/1     Completed   0          4m46s
hadoop-cli-7.2.11-hf4-62chh-sj7v8                0/1     Completed   0          4m46s
hadoop-cli-7.2.8-hf1-v6yy0j-z7lpj                0/1     Completed   0          4m46s
livelog-0                                        2/2     Running     0          6m54s
livelog-publisher-8nptf                          2/2     Running     0          6m54s
livelog-publisher-jb5ph                          2/2     Running     0          6m54s
livelog-publisher-m87d9                          2/2     Running     0          6m54s
model-metrics-77bd8c6575-f4j8w                   1/1     Running     3          6m54s
model-metrics-db-0                               1/1     Running     0          6m54s
model-proxy-f5ff968cd-czv2k                      2/2     Running     0          6m54s
prometheus-postgres-exporter-6f785b7755-6j68b    1/1     Running     0          6m54s
runtime-addon-trigger-2.0.28-b66-txfcj           0/1     Completed   0          6m44s
runtime-initial-repo-inserter-2.0.28-b66-j2psv   0/1     Completed   0          6m44s
runtime-manager-7775c48879-9kwgt                 2/2     Running     0          6m54s
s2i-builder-7f4879fb78-22qgv                     2/2     Running     0          6m54s
s2i-builder-7f4879fb78-nd7rs                     2/2     Running     0          6m54s
s2i-builder-7f4879fb78-z4zqt                     2/2     Running     0          6m54s
s2i-client-6f596848d8-kbkjb                      2/2     Running     0          6m54s
s2i-git-server-0                                 2/2     Running     0          6m54s
s2i-queue-0                                      2/2     Running     0          6m54s
s2i-registry-auth-7556777cbf-7nfpf               2/2     Running     0          6m54s
s2i-registry-c6787ccd-6rh4p                      2/2     Running     0          6m54s
s2i-server-bb479d45b-h8ljs                       2/2     Running     0          6m54s
secret-generator-0                               2/2     Running     0          6m54s
spark247-13-hf2-nvcpz-p4bmw                      0/1     Completed   0          4m46s
spark311-13-hf2-ft7g6a-nqtd6                     0/1     Completed   0          4m46s
tcp-ingress-controller-69c976d9d9-ctpsw          2/2     Running     0          6m54s
usage-reporter-6f67b66bd9-mm5df                  2/2     Running     0          6m54s
web-66476566f9-2jjs5                             2/2     Running     0          6m54s
web-66476566f9-n6lrc                             2/2     Running     0          6m54s
web-66476566f9-t5dxg                             2/2     Running     0          6m54s
[root@ocpbastion ~]# oc get pvc
NAME                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
livelog-data-livelog-0                  Bound    pvc-4054e760-535c-4667-8820-6bbedde65022   1Ti        RWO            ocs-storagecluster-ceph-rbd   7m2s
model-metrics-data-model-metrics-db-0   Bound    pvc-0b96f2d7-5341-4d55-8992-a4ced2aba6bb   100Gi      RWO            ocs-storagecluster-ceph-rbd   7m2s
persist-dir-secret-generator-0          Bound    pvc-9a40c7cb-649b-4453-b377-e98f65316b3b   10Mi       RWO            ocs-storagecluster-ceph-rbd   7m2s
postgres-data-versioned-db-0            Bound    pvc-c7142527-fcbf-47dd-b29e-570edee3bf2b   1Ti        RWO            ocs-storagecluster-ceph-rbd   7m2s
projects-pvc                            Bound    projects-share-ws1                         1Ti        RWX                                          7m3s
registry-pvc                            Bound    pvc-97381aea-f0b6-42a2-9638-883f9cc1fb5f   1Ti        RWO            ocs-storagecluster-ceph-rbd   7m3s
s2i-git-server-repos-s2i-git-server-0   Bound    pvc-20671830-671c-4c76-9538-1ea5d083058a   1Ti        RWO            ocs-storagecluster-ceph-rbd   7m2s
s2i-queue-pvc                           Bound    pvc-ee89ea8f-7a99-4f7b-9995-03c49b9a1e0c   2Gi        RWO            ocs-storagecluster-ceph-rbd   7m3s


CDE:

[root@ocpbastion ~]# oc -n dex-base-hqxd5d59 get pvc
NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
dex-base-db-pvc    Bound    pvc-55f1024a-c2b0-4a73-8370-1413ac7ac4cf   100Gi      RWO            ocs-storagecluster-ceph-rbd   95s
dex-base-grafana   Bound    pvc-9fda6a03-06bd-4f8f-9313-84d5c11d2d0c   10Gi       RWO            ocs-storagecluster-ceph-rbd   95s
[root@ocpbastion ~]# oc -n dex-base-hqxd5d59 get pods
NAME                                           READY   STATUS    RESTARTS   AGE
cdp-cde-embedded-db-0                          1/1     Running   0          97s
dex-base-configs-manager-6c57c88f69-nwrqk      2/2     Running   0          97s
dex-base-dex-downloads-5c9697dbb4-wm6l4        1/1     Running   0          97s
dex-base-grafana-846c88d599-6xj59              1/1     Running   0          97s
dex-base-hqxd5d59-controller-7bd8d95f4-rgb5p   1/1     Running   0          97s
dex-base-knox-6779786dd5-6hz6t                 1/1     Running   0          97s
dex-base-management-api-797bb8d8f5-nqsdg       1/1     Running   0          97s
fluentd-forwarder-688cb7bc7b-xzbn8             1/1     Running   0          97s


[root@ocpbastion ~]# oc -n dex-app-dzxzpmsh get pvc
NAME                             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
airflow-dags                     Pending                                      nfs            7s
airflow-logs                     Pending                                      nfs            7s
dex-app-dzxzpmsh-livystate-pvc   Pending                                      nfs            7s
dex-app-dzxzpmsh-safari-pvc      Pending                                      nfs            7s
dex-app-dzxzpmsh-storage-pvc     Pending                                      nfs            7s
[root@ocpbastion ~]# oc -n dex-app-dzxzpmsh get pods
NAME                                                  READY   STATUS    RESTARTS   AGE
dex-app-dzxzpmsh-airflow-scheduler-5bd7f969cc-qncpq   0/1     Pending   0          15s
dex-app-dzxzpmsh-airflow-web-76d8b987c9-2rssk         0/1     Pending   0          14s
dex-app-dzxzpmsh-airflowapi-6877c7d55d-2gtl4          0/2     Pending   0          15s
dex-app-dzxzpmsh-api-886b7df76-6gxvf                  0/1     Pending   0          15s
dex-app-dzxzpmsh-livy-5f5b7d95db-nctdk                0/1     Pending   0          15s
dex-app-dzxzpmsh-safari-957b79f49-bs7hq               0/1     Pending   0          15s





ECS:

[root@ecsmaster1 ~]# kubectl -n compute-1656482548-g2jx get pods
NAME                            READY   STATUS    RESTARTS   AGE
das-webapp-0                    1/1     Running   0          5m22s
hiveserver2-0                   1/1     Running   0          5m22s
huebackend-0                    1/1     Running   0          5m22s
huefrontend-589cb7546f-c5d9n    1/1     Running   0          5m22s
query-coordinator-0-0           1/1     Running   0          5m18s
query-coordinator-0-1           1/1     Running   0          5m18s
query-coordinator-0-2           1/1     Running   0          5m18s
query-coordinator-0-3           1/1     Running   0          5m18s
query-executor-0-0              1/1     Running   0          5m18s
query-executor-0-1              1/1     Running   0          5m18s
query-executor-0-2              1/1     Running   0          5m18s
query-executor-0-3              1/1     Running   0          5m18s
standalone-compute-operator-0   1/1     Running   0          5m22s
usage-monitor-97df8f4d4-6qqpf   1/1     Running   0          5m22s


[root@ecsmaster1 ~]# kubectl -n compute-1656482548-g2jx get pvc
NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
query-executor-1656482568-volume-query-executor-0-0   Bound    pvc-656b1bb8-57f8-44dd-8396-a83c4b3f13eb   100Gi      RWO            local-path     2m56s
query-executor-1656482568-volume-query-executor-0-1   Bound    pvc-8716ac21-3dc4-42c6-9a32-1e84c58b8845   100Gi      RWO            local-path     2m56s
query-executor-1656482568-volume-query-executor-0-2   Bound    pvc-7bdff9fe-8135-494a-b0a0-4933d94e4f90   100Gi      RWO            local-path     2m56s
query-executor-1656482568-volume-query-executor-0-3   Bound    pvc-e9318ee8-66fe-4c4a-9a0e-576163d5fa1b   100Gi      RWO            local-path     2m56s

[root@ecsmaster1 ~]# kubectl describe pv pvc-8716ac21-3dc4-42c6-9a32-1e84c58b8845  | grep hostname
    Term 0:        kubernetes.io/hostname in [ecsworker3.cdpkvm.cldr]
[root@ecsmaster1 ~]# kubectl describe pv pvc-e9318ee8-66fe-4c4a-9a0e-576163d5fa1b | grep hostname
    Term 0:        kubernetes.io/hostname in [ecsworker3.cdpkvm.cldr]

[root@ecsmaster1 ~]# kubectl get sc
NAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path           rancher.io/local-path   Delete          WaitForFirstConsumer   false                  38d
longhorn (default)   driver.longhorn.io      Delete          Immediate              true                   38d
longhorn-nfs         nfs.longhorn.io         Delete          Immediate              false                  38d


[root@ecsworker3 ~]# tree /localpath/
/localpath/
|-- local-storage
|   |-- pvc-8716ac21-3dc4-42c6-9a32-1e84c58b8845_compute-1656482548-g2jx_query-executor-1656482568-volume-query-executor-0-1
|   |   `-- llap-2396830618975518217
|   `-- pvc-e9318ee8-66fe-4c4a-9a0e-576163d5fa1b_compute-1656482548-g2jx_query-executor-1656482568-volume-query-executor-0-3
|       `-- llap-5697099011548069627
`-- longhorntmp
    |-- longhorn-disk.cfg
    `-- replicas

7 directories, 1 file




control plane:
[root@ocpbastion ~]# oc adm top pods -n yunikorn
NAME                                             CPU(cores)   MEMORY(bytes)   
yunikorn-admission-controller-58985d458f-2sjvk   2m           19Mi            
yunikorn-scheduler-59886d55b6-h99m2              1601m        239Mi           
[root@ocpbastion ~]# oc adm top pods -n cdp
NAME                                                              CPU(cores)   MEMORY(bytes)   
cdp-release-alert-admin-service-957bfcb58-8wcr4                   2m           88Mi            
cdp-release-classic-clusters-648cc99645-kr5j7                     25m          1058Mi          
cdp-release-cluster-access-manager-69b56fc4db-shvwv               2m           58Mi            
cdp-release-cluster-proxy-1.0.0-7cb6bfbbbd-zwzpr                  21m          940Mi           
cdp-release-cpx-liftie-6568b4669c-tcfwk                           0m           81Mi            
cdp-release-dex-cp-598965f46c-dzg9z                               2m           94Mi            
cdp-release-dmx-55bbfd8cd9-wlqml                                  6m           987Mi           
cdp-release-dps-gateway-1.0-5d58765b8-p5t5m                       9m           114Mi           
cdp-release-dps-gateway-1.0-5d58765b8-scrh2                       7m           110Mi           
cdp-release-dwx-server-76476fc4b5-7nb72                           4m           133Mi           
cdp-release-dwx-ui-865f4998d7-ct5sq                               0m           22Mi            
cdp-release-dwx-ui-865f4998d7-mn7cq                               0m           22Mi            
cdp-release-grafana-54d8b7cf5-64nnm                               6m           193Mi           
cdp-release-logger-alert-receiver-f56f64dc8-4wpzh                 0m           69Mi            
cdp-release-metrics-server-exporter-65f65c54b-2bg7j               1m           74Mi            
cdp-release-monitoring-app-6f6c9b8fb4-dkzdd                       0m           71Mi            
cdp-release-monitoring-metricproxy-b5f85766c-5js7j                0m           71Mi            
cdp-release-monitoring-metricproxy-b5f85766c-6j2dl                0m           68Mi            
cdp-release-monitoring-pvcservice-757bf9bdd4-rmrx4                1m           98Mi            
cdp-release-prometheus-alertmanager-0                             3m           61Mi            
cdp-release-prometheus-alertmanager-1                             1m           59Mi            
cdp-release-prometheus-kube-state-metrics-65c8fd786b-782w2        0m           40Mi            
cdp-release-prometheus-server-c66797d5f-gqlsx                     7m           213Mi           
cdp-release-reloader-676bb9945c-pmjfb                             0m           22Mi            
cdp-release-resource-pool-manager-67dbdfcdd8-6bg9f                6m           51Mi            
cdp-release-thunderhead-cdp-private-authentication-consoletp42g   0m           45Mi            
cdp-release-thunderhead-cdp-private-commonconsole-7ffdf77d4qhnw   0m           44Mi            
cdp-release-thunderhead-cdp-private-environments-console-5ggnpv   0m           44Mi            
cdp-release-thunderhead-compute-api-6b795564fd-s9jj2              0m           779Mi           
cdp-release-thunderhead-consoleauthenticationcdp-8c765dd9725plv   0m           799Mi           
cdp-release-thunderhead-de-api-688c9b6b7-4w5xn                    1m           725Mi           
cdp-release-thunderhead-environment-69b8cd49c5-t878p              10m          1209Mi          
cdp-release-thunderhead-environments2-api-6767bdd568-59r8s        5m           897Mi           
cdp-release-thunderhead-iam-api-7565b47677-njsqt                  0m           816Mi           
cdp-release-thunderhead-iam-console-cdd6dd84f-cp98q               0m           44Mi            
cdp-release-thunderhead-kerberosmgmt-api-68cdd5b56-s7qzc          0m           775Mi           
cdp-release-thunderhead-ml-api-8f488bc95-jhq6k                    1m           765Mi           
cdp-release-thunderhead-resource-management-console-6f8dcfsb7nk   0m           44Mi            
cdp-release-thunderhead-sdx2-api-7476f74476-g7lr7                 1m           747Mi           
cdp-release-thunderhead-servicediscoverysimple-bdf5fb467-8whfw    1m           723Mi           
cdp-release-thunderhead-usermanagement-private-5f494b96dd-vwbwp   14m          958Mi           
dp-mlx-control-plane-app-5ff67bdf77-76t46                         0m           85Mi            
dp-mlx-control-plane-app-health-poller-66fd654d9f-zwvtm           0m           55Mi            
fluentd-aggregator-0                                              10m          3551Mi          
snmp-notifier-7498cc86bc-rk4g6                                    0m           35Mi             
[root@ocpbastion ~]# oc adm top pods -n cdp-env-1-d9aab9ef-monitoring-platform 
NAME                                                        CPU(cores)   MEMORY(bytes)   
monitoring-cm-health-exporter-7944475f99-h7l6b              4m           876Mi           
monitoring-logger-alert-receiver-79fd679b8d-2c7w6           1m           117Mi           
monitoring-metrics-server-exporter-5dc47fbbb9-bq4zb         2m           131Mi           
monitoring-platform-proxy-6777989db7-lxkds                  2m           101Mi           
monitoring-prometheus-alertmanager-0                        3m           112Mi           
monitoring-prometheus-alertmanager-1                        3m           110Mi           
monitoring-prometheus-kube-state-metrics-54b79bd9c8-jpqb7   0m           85Mi            
monitoring-prometheus-pushgateway-99499d6d6-czgcl           1m           89Mi            
monitoring-prometheus-server-6f74fdbc9c-mzvlh               6m           221Mi           
snmp-notifier-765477fc6c-l2rx9                              1m           89Mi   

CML:
[root@ocpbastion ~]# oc adm top pods -n ws1
NAME                                            CPU(cores)   MEMORY(bytes)   
api-786d4c8876-lj6dh                            0m           25Mi            
cron-5df9b4d86d-pwj46                           0m           48Mi            
db-0                                            6m           92Mi            
ds-cdh-client-6bcdb4bfcb-2qjz6                  0m           822Mi           
ds-operator-69d5d4c9db-6pc72                    0m           66Mi            
ds-reconciler-6b894fd4b4-mqmv4                  6m           107Mi           
ds-vfs-b489bcf97-flkgb                          0m           55Mi            
feature-flags-6bfcbbf4c4-t7gvl                  5m           63Mi            
fluentd-forwarder-6f7c84878d-g587g              2m           143Mi           
governance-759d59dd7-pr9qh                      0m           1025Mi          
livelog-0                                       0m           61Mi            
livelog-publisher-8z7vk                         0m           44Mi            
livelog-publisher-h5z9f                         0m           46Mi            
livelog-publisher-slkbf                         0m           43Mi            
model-metrics-6df595b558-xb9rt                  0m           15Mi            
model-metrics-db-0                              4m           23Mi            
model-proxy-6ccb778c9-km86k                     0m           70Mi            
prometheus-postgres-exporter-5956d48756-gkdp9   0m           10Mi            
runtime-manager-64db747ff6-h6m8p                0m           42Mi            
s2i-builder-7b4f878b5d-h5vl9                    0m           340Mi           
s2i-builder-7b4f878b5d-s7rmk                    0m           31Mi            
s2i-builder-7b4f878b5d-svbms                    0m           31Mi            
s2i-client-5f966bc7d9-7jpw7                     0m           69Mi            
s2i-git-server-0                                0m           43Mi            
s2i-queue-0                                     21m          248Mi           
s2i-registry-7fd6658c6c-mzsf7                   1m           63Mi            
s2i-registry-auth-54dfb46556-zc7k4              1m           33Mi            
s2i-server-bd7954c8d-2jbf8                      0m           45Mi            
secret-generator-0                              1m           55Mi            
tcp-ingress-controller-c6c56d7d6-vc72w          0m           36Mi            
usage-reporter-675f6488cc-dmngb                 0m           29Mi            
web-6b9459d445-kjppb                            2m           217Mi           
web-6b9459d445-tkxj7                            0m           242Mi           
web-6b9459d445-zzw7b                            0m           160Mi           
[root@ocpbastion ~]# oc adm top pods -n ws1-user-1
NAME                                           CPU(cores)   MEMORY(bytes)   
churn-model-api-endpoint-3-1-8fc6dc846-4bfr6   2m           261Mi           
ftkstf3074bmexmw                               11m          369Mi 


CDW-impala:
[root@ocpbastion ~]# oc adm top pods -n shared-services
NAME                                    CPU(cores)   MEMORY(bytes)   
log-router-9mdxr                        1416m        324Mi           
log-router-b4jkp                        975m         297Mi           
log-router-bh2tc                        991m         327Mi           
openshift-idling-controller-manager-0   1m           11Mi
[root@ocpbastion ~]# oc adm top pods -n warehouse-1656567061-jjzw
NAME                    CPU(cores)   MEMORY(bytes)   
das-event-processor-0   29m          2850Mi          
metastore-0             39m          3033Mi          
metastore-1             16m          2981Mi 
[root@ocpbastion ~]# oc adm top pods -n impala-1656935869-bvq4
NAME                                 CPU(cores)   MEMORY(bytes)   
catalogd-76b9c5cd86-vgsqp            26m          488Mi           
coordinator-0                        565m         594Mi           
huebackend-0                         121m         216Mi           
impala-autoscaler-855b978c68-s7mfr   15m          14Mi            
impala-executor-000-0                15m          422Mi           
impala-executor-000-1                7m           433Mi           
statestored-8689bbd5db-5pzsx         8m           12Mi 

CDW-hive:
[root@ocpbastion ~]# oc adm top pods -n shared-services
NAME                                    CPU(cores)   MEMORY(bytes)   
log-router-9mdxr                        1416m        324Mi           
log-router-b4jkp                        975m         297Mi           
log-router-bh2tc                        991m         327Mi           
openshift-idling-controller-manager-0   1m           11Mi 
[root@ocpbastion ~]# oc adm top pods -n compute-1656936094-2st6
NAME                             CPU(cores)   MEMORY(bytes)   
das-webapp-0                     18m          424Mi           
hiveserver2-0                    31m          1300Mi          
huebackend-0                     13m          217Mi           
huefrontend-57f8d69bc7-g8lhp     0m           9Mi             
query-coordinator-0-0            41m          703Mi           
query-coordinator-0-1            43m          703Mi           
query-executor-0-0               41m          949Mi           
query-executor-0-1               52m          975Mi           
standalone-compute-operator-0    0m           27Mi            
usage-monitor-6c4fd58bc4-cgtkk   3m           12Mi 

CDE:
[root@ocpbastion ~]# oc -n dex-base-g86mlq9q  adm top pods
NAME                                           CPU(cores)   MEMORY(bytes)   
cdp-cde-embedded-db-0                          8m           505Mi           
dex-base-configs-manager-5f74f57564-64dvh      0m           289Mi           
dex-base-dex-downloads-779d65779c-p2dsr        0m           23Mi            
dex-base-g86mlq9q-controller-c4465b645-ft944   11m          294Mi           
dex-base-grafana-6c65dffc48-2w5lh              0m           24Mi            
dex-base-knox-5d6688b4-cf6wk                   1m           420Mi           
dex-base-management-api-586dfb4c94-d246f       0m           247Mi           
fluentd-forwarder-757d65d65c-kghfm             0m           91Mi 

[root@ocpbastion ~]# oc -n dex-app-p5q6xqdq get pods
NAME                                                 READY   STATUS    RESTARTS   AGE
dex-app-p5q6xqdq-airflow-scheduler-b78794fc8-v5wt8   0/1     Pending   0          61s
dex-app-p5q6xqdq-airflow-web-57f878b97f-vg74v        0/1     Pending   0          61s
dex-app-p5q6xqdq-airflowapi-64d4bf6767-b7pt2         0/2     Pending   0          61s
dex-app-p5q6xqdq-api-7bccbf4877-z9vj5                0/1     Pending   0          61s
dex-app-p5q6xqdq-livy-7b69c9d7f9-s4d44               0/1     Pending   0          61s
dex-app-p5q6xqdq-safari-5745ddd96d-rjblh             0/1     Pending   0          61s



CDW (not low resource mode)

[root@ocpbastion ~]# oc -n impala-1657000326-zvn5 describe pvc scratch-cache-volume-coordinator-0 
Name:          scratch-cache-volume-coordinator-0
Namespace:     impala-1657000326-zvn5
StorageClass:  cdw-disk
Status:        Pending
Volume:        
Labels:        app=coordinator
Annotations:   <none>
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      
Access Modes:  
VolumeMode:    Filesystem
Used By:       coordinator-0
Events:
  Type    Reason                Age               From                         Message
  ----    ------                ----              ----                         -------
  Normal  WaitForFirstConsumer  60s               persistentvolume-controller  waiting for first consumer to be created before binding
  Normal  WaitForPodScheduled   1s (x4 over 46s)  persistentvolume-controller  waiting for pod coordinator-0 to be scheduled
[root@ocpbastion ~]# oc -n impala-1657000326-zvn5 get pvc scratch-cache-volume-coordinator-0 -o yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  creationTimestamp: "2022-07-05T05:52:25Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app: coordinator
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .: {}
          f:app: {}
      f:spec:
        f:accessModes: {}
        f:resources:
          f:requests:
            .: {}
            f:storage: {}
        f:storageClassName: {}
        f:volumeMode: {}
      f:status:
        f:phase: {}
    manager: kube-controller-manager
    operation: Update
    time: "2022-07-05T05:52:25Z"
  name: scratch-cache-volume-coordinator-0
  namespace: impala-1657000326-zvn5
  resourceVersion: "5586288"
  selfLink: /api/v1/namespaces/impala-1657000326-zvn5/persistentvolumeclaims/scratch-cache-volume-coordinator-0
  uid: f385a97c-dfd0-44f7-ade7-265fdc8518c8
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 600Gi
  storageClassName: cdw-disk
  volumeMode: Filesystem
status:
  phase: Pending

[root@ocpbastion ~]# oc -n impala-1657000326-zvn5 get pvc scratch-cache-volume-impala-executor-000-0 -o yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  creationTimestamp: "2022-07-05T05:52:25Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app: impala-executor
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .: {}
          f:app: {}
      f:spec:
        f:accessModes: {}
        f:resources:
          f:requests:
            .: {}
            f:storage: {}
        f:storageClassName: {}
        f:volumeMode: {}
      f:status:
        f:phase: {}
    manager: kube-controller-manager
    operation: Update
    time: "2022-07-05T05:52:25Z"
  name: scratch-cache-volume-impala-executor-000-0
  namespace: impala-1657000326-zvn5
  resourceVersion: "5586245"
  selfLink: /api/v1/namespaces/impala-1657000326-zvn5/persistentvolumeclaims/scratch-cache-volume-impala-executor-000-0
  uid: 1926bb4e-ac1c-4357-9127-bf68eebbd640
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 600Gi
  storageClassName: cdw-disk
  volumeMode: Filesystem
status:
  phase: Pending

impala:
# oc -n impala-1657000326-zvn5 get pod impala-executor-000-0  -o yaml
    resources:
      limits:
        memory: 116736M
      requests:
        cpu: "14"
        memory: 116736M

# oc -n impala-1657000326-zvn5 get pod coordinator-0  -o yaml
      limits:
        memory: 102400M
      requests:
        cpu: "8"
        memory: 102400M        
        
hive:
# oc -n compute-1657001663-wz6k get pod query-coordinator-0-0  -o yaml
    resources:
      limits:
        cpu: "2"
        memory: 4096M
      requests:
        cpu: "2"
        memory: 4096M

# oc -n compute-1657001663-wz6k get pod query-executor-0-0 -o yaml
    resources:
      limits:
        cpu: "12"
        memory: 116736M
      requests:
        cpu: "12"
        memory: 116736M



cp -r /sdx-templates/* /sdx-templates-shared
echo "hive/dwx-env-6hcbzr" > /sdx-templates-shared/hive.krb5template
cp /tmp/hive/conf/* /etc/hive/conf/

/usr/local/bin/config-merger /tmp/hive/conf/hive-site.xml /mnt/config/current/hive-conf/hive-site.xml /etc/hive/conf/hive-site.xml -wf2 hive.metastore.kerberos.principal -set hive.server2.authentication.kerberos.principal='${SERVICE_PRINCIPAL}' -set hive.server2.authentication.kerberos.keytab=/mnt/config/current/hive -set hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.ZooKeeperTokenStore -set hive.metastore.sasl.enabled=true

/usr/local/bin/config-merger /tmp/hadoop/conf/core-site.xml /mnt/config/current/hive-conf/core-site.xml /etc/hadoop/conf/core-site.xml
cp /mnt/config/current/hive-conf/yarn-site.xml /etc/hadoop/conf/yarn-site.xml
cp /mnt/config/current/hive-conf/hdfs-site.xml /etc/hadoop/conf/hdfs-site.xml
export OZONE_SITE_LOCATION=/mnt/config/current/hive-conf/ozone-site.xml; if [ -f "$OZONE_SITE_LOCATION" ]; then cp $OZONE_SITE_LOCATION /etc/hadoop/conf/ozone-site.xml; else echo "File does not exist: $OZONE_SITE_LOCATION"; fi;
 cp /mnt/config/current/krb5.conf /etc/krb5.conf && klist -kt /mnt/config/current/hive | sed -n -e '$p'|awk '{ print $NF }' | sed 's/^/export SERVICE_PRINCIPAL=/' > /mnt/config/current/principal.env
 
 echo "export RANGER_URL=\"$(awk '/ranger.plugin.hive.policy.rest.url/{getline; print}' /mnt/config/current/ranger-hive-security.xml | sed -E 's/^.*<value>(.*)<\/value>$/\1/g' | sed 's/\/$//' )\"" >> /mnt/config/current/principal.env
 
 echo "export RANGER_CLUSTER_NAME=\"$(awk '/ranger.plugin.hive.access.cluster.name/{getline; print}' /mnt/config/current/ranger-hive-security.xml | cut -d '>' -f 2 | cut -f 1 -d'<')\"" >> /mnt/config/current/principal.env
 
 source /mnt/config/current/principal.env; find /etc/hive/conf/ -type f | while read file; do cat $file | envsubst '$SERVICE_PRINCIPAL' | envsubst '$HADOOP_USER_NAME' | envsubst '$WAREHOUSE_DIR' > $file.new; mv $file.new $file; done
 
 
  while ! nc -zv -w 2 metastore-service 9083; do echo "waiting for metastore-service to connect on 9083.."; sleep 5; done;
  
   /sys-entrypoint.sh

  
CREATE EXTERNAL TABLE db1.tmp(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  COMMENT 'Residents Details'
  ROW FORMAT DELIMITED
  FIELDS TERMINATED BY ','
  STORED AS TEXTFILE
  LOCATION 'hdfs:/tmp/sampledata'

CREATE TABLE db1.avro(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS avro
  
INSERT INTO TABLE avro SELECT * FROM tmp

CREATE TABLE db1.parquet(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS parquet
  
CREATE TABLE db1.orc(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS orc  

CREATE EXTERNAL TABLE db1.tmp2(
  Caller bigint, Called bigint)
  ROW FORMAT DELIMITED
  FIELDS TERMINATED BY ','
  STORED AS TEXTFILE
  LOCATION 'hdfs:/tmp/callrecords'

CREATE TABLE db1.orc2(   
  Caller bigint, Called bigint)
  STORED AS orc    

CREATE TABLE db1.avro2(
  one string)
  STORED AS avro
  TBLPROPERTIES ('avro.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"string"}]}');
INSERT INTO TABLE avro2 SELECT first FROM tmp3 
SELECT * from avro2
ALTER TABLE db1.avro2 SET TBLPROPERTIES 
  ('avro.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"string"},
  {"name":"two", "type":["null","long"], "default":null}
  ]}')
ALTER TABLE db1.avro2 ADD COLUMNS (two bigint)

CREATE TABLE db1.parquet2(
  one string)
  STORED AS PARQUET
  TBLPROPERTIES ('parquet.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"binary"}]}');
INSERT INTO TABLE parquet2 SELECT first FROM tmp3 
SELECT * from parquet2
ALTER TABLE db1.parquet2 SET TBLPROPERTIES 
  ('parquet.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"binary"},
  {"name":"two", "type":"INT64"}
  ]}')
ALTER TABLE db1.parquet2 ADD COLUMNS (two bigint)
INSERT INTO TABLE parquet2 (two) SELECT second FROM tmp3 
INSERT INTO TABLE parquet2 SELECT * FROM tmp3

CREATE TABLE db1.orc2(
  one string)
  STORED AS PARQUET
  TBLPROPERTIES ('orc.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"binary"}]}');
INSERT INTO TABLE orc2 SELECT first FROM tmp3
SELECT * from parquet2
ALTER TABLE db1.orc2 SET TBLPROPERTIES 
  ('orc.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"binary"},
  {"name":"two", "type":"bigint"}
  ]}')
ALTER TABLE db1.orc2 ADD COLUMNS (two bigint)
INSERT INTO TABLE orc2 (two) SELECT second FROM tmp3 

CREATE TABLE db1.parquet(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS parquet
  TBLPROPERTIES ('parquet.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"binary"},
  {"name":"two", "type":"binary"},
  {"name":"three", "type":"INT64"},
  {"name":"four", "type":"date"},
  {"name":"five", "type":"INT32"},
  {"name":"six", "type":"INT32"},
  {"name":"seven", "type":"binary"}
  ]}')
INSERT INTO TABLE parquet SELECT * FROM tmp


CREATE TABLE db1.avro(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS avro
  TBLPROPERTIES ('avro.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"FirstName", "type":"string"},
  {"name":"LastName", "type":"string"},
  {"name":"MSISDN", "type":"long"},
  {"name":"DOB", "type":"string"},
  {"name":"age", "type":"int"},
  {"name":"Postcode", "type":"int"},
  {"name":"City", "type":"string"}
  ]}')
INSERT INTO TABLE avro SELECT * FROM tmp 

CREATE TABLE db1.orc(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS orc
  TBLPROPERTIES ('orc.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"binary"},
  {"name":"two", "type":"binary"},
  {"name":"three", "type":"bigint"},
  {"name":"four", "type":"binary"},
  {"name":"five", "type":"int"},
  {"name":"six", "type":"int"},
  {"name":"seven", "type":"binary"}
  ]}')
INSERT INTO TABLE orc SELECT * FROM tmp

CREATE TABLE db1.parquet2(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS parquet
  TBLPROPERTIES (
  "parquet.compression"="SNAPPY",
  'parquet.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"string"},
  {"name":"two", "type":"string"},
  {"name":"three", "type":"long"},
  {"name":"four", "type":"date"},
  {"name":"five", "type":"long"},
  {"name":"six", "type":"int"},
  {"name":"seven", "type":"string"}
  ]}')
INSERT INTO TABLE parquet2 SELECT * FROM tmp  


CREATE TABLE db1.orc2(
  FirstName string, LastName string,    
  MSISDN bigint, DOB date, age int,
  Postcode int, City string)
  STORED AS orc
  TBLPROPERTIES (
  "orc.compression"="SNAPPY",
  'orc.schema.literal'='{
  "name": "sample1",
  "type": "record",
  "fields": [
  {"name":"one", "type":"binary"},
  {"name":"two", "type":"binary"},
  {"name":"three", "type":"bigint"},
  {"name":"four", "type":"binary"},
  {"name":"five", "type":"int"},
  {"name":"six", "type":"int"},
  {"name":"seven", "type":"binary"}
  ]}')
INSERT INTO TABLE orc SELECT * FROM tmp


SELECT AVG(Age) AS Avgage FROM tmp
SELECT lastname, firstname, msisdn FROM tmp WHERE age >= 30 AND age <= 60

select A.lastname,A.firstname,A.msisdn from orc A where A.msisdn in 
(select B.caller from orc2 B where B.called = 61919437 )

CML:
# # Word counts
# 
# This example shows how to count the occurrences of each word in a text file.

from __future__ import print_function
import sys, re
from operator import add
from pyspark.sql import SparkSession

spark = SparkSession\
    .builder\
    .appName("PythonWordCount")\
    .getOrCreate()

# Add the data file to hdfs.
!hdfs dfs -put resources/cgroup-v2.txt /tmp

lines = spark.read.text("/tmp/cgroup-v2.txt").rdd.map(lambda r: r[0])
counts = lines.flatMap(lambda x: x.split(' ')) \
              .map(lambda x: (x, 1)) \
              .reduceByKey(add) \
              .sortBy(lambda x: x[1], False)
output = counts.collect()
for (word, count) in output:
    print("%s: %i" % (word, count))

spark.stop()





[root@ocpbastion ~]# oc get networkpolicy
NAME                           POD-SELECTOR   AGE
allow-from-openshift-ingress   <none>         24h
allow-from-workspace2          <none>         24h
allow-from-workspace2-user-1   <none>         24h
(reverse-i-search)`-0': oc exec -ti huebackend^C  -- /bin/sh
[root@ocpbastion ~]# oc get networkpolicy allow-from-openshift-ingress -o yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: "2022-07-07T08:34:28Z"
  generation: 1
  managedFields:
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:ingress: {}
        f:policyTypes: {}
    manager: operator
    operation: Update
    time: "2022-07-07T08:34:28Z"
  name: allow-from-openshift-ingress
  namespace: workspace2-user-1
  resourceVersion: "6934442"
  selfLink: /apis/networking.k8s.io/v1/namespaces/workspace2-user-1/networkpolicies/allow-from-openshift-ingress
  uid: 565d34d6-f2e3-4cfd-9116-25da69927804
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
[root@ocpbastion ~]# oc get networkpolicy allow-from-workspace2 -o yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: "2022-07-07T08:34:28Z"
  generation: 1
  managedFields:
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:ingress: {}
        f:policyTypes: {}
    manager: operator
    operation: Update
    time: "2022-07-07T08:34:28Z"
  name: allow-from-workspace2
  namespace: workspace2-user-1
  resourceVersion: "6934434"
  selfLink: /apis/networking.k8s.io/v1/namespaces/workspace2-user-1/networkpolicies/allow-from-workspace2
  uid: 10fe39ef-f044-4926-bd47-e629781703d4
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: workspace2
  podSelector: {}
  policyTypes:
  - Ingress
[root@ocpbastion ~]# oc get networkpolicy allow-from-workspace2-user-1 -o yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: "2022-07-07T08:34:28Z"
  generation: 1
  managedFields:
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:ingress: {}
        f:policyTypes: {}
    manager: operator
    operation: Update
    time: "2022-07-07T08:34:28Z"
  name: allow-from-workspace2-user-1
  namespace: workspace2-user-1
  resourceVersion: "6934428"
  selfLink: /apis/networking.k8s.io/v1/namespaces/workspace2-user-1/networkpolicies/allow-from-workspace2-user-1
  uid: fa1dac31-8779-420b-b1a7-817a62f0c125
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: workspace2-user-1
  podSelector: {}
  policyTypes:
  - Ingress
  
  
Before provision CDP:

[root@ocpbastion ~]# oc adm top nodes
NAME                        CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master01.ocp4.cdpkvm.cldr   1865m        12%    12892Mi         10%       
master02.ocp4.cdpkvm.cldr   2021m        13%    15535Mi         12%       
master03.ocp4.cdpkvm.cldr   1822m        11%    10980Mi         8% 


After provision CDP:

[root@ocpbastion ~]# oc adm top nodes
NAME                        CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master01.ocp4.cdpkvm.cldr   3053m        19%    13655Mi         10%       
master02.ocp4.cdpkvm.cldr   4379m        28%    19697Mi         15%       
master03.ocp4.cdpkvm.cldr   1962m        12%    12972Mi         10% 

[root@ocpbastion ~]# oc adm top pods -n cdp
NAME                                                              CPU(cores)   MEMORY(bytes)   
cdp-embedded-db-0                                                 8m           247Mi           
cdp-release-alert-admin-service-774cd8b775-dbh8g                  0m           78Mi            
cdp-release-classic-clusters-77b766756d-kz4cx                     35m          930Mi           
cdp-release-classic-clusters-84c7f747d7-5ftmc                     35m          909Mi           
cdp-release-cluster-access-manager-647655cccf-lghjp               4m           15Mi            
cdp-release-cluster-access-manager-f67bb8d94-s6hcv                1m           32Mi            
cdp-release-cluster-proxy-1.0.0-5c4c696fc5-f4kjd                  49m          731Mi           
cdp-release-cluster-proxy-1.0.0-7954c8c54-j9mpn                   37m          827Mi           
cdp-release-cpx-liftie-7db79b76b-9fv77                            1m           61Mi            
cdp-release-dex-cp-5cd969bf4c-8p8h6                               20m          88Mi            
cdp-release-dmx-65858fbc8c-tfsqt                                  0m           45Mi            
cdp-release-dps-gateway-1.0-5c7987bbf4-28p4p                      4m           69Mi            
cdp-release-dps-gateway-1.0-5c7987bbf4-cw4gq                      5m           67Mi            
cdp-release-dwx-server-575775bdd-4prqb                            0m           118Mi           
cdp-release-dwx-ui-746645b8b7-f4sgt                               0m           22Mi            
cdp-release-dwx-ui-746645b8b7-kqpnz                               0m           22Mi            
cdp-release-grafana-6687fdf4d4-zmtnj                              1m           116Mi           
cdp-release-logger-alert-receiver-79c57c846d-pffkc                0m           56Mi            
cdp-release-metrics-server-exporter-fbf976546-d6n6r               2m           63Mi            
cdp-release-monitoring-app-65b9b496dc-qb8r4                       0m           54Mi            
cdp-release-monitoring-metricproxy-77fdb47854-j97xg               0m           51Mi            
cdp-release-monitoring-metricproxy-77fdb47854-xz7lt               0m           50Mi            
cdp-release-monitoring-pvcservice-f844b484c-64thh                 1m           54Mi            
cdp-release-prometheus-alertmanager-0                             3m           46Mi            
cdp-release-prometheus-alertmanager-1                             2m           45Mi            
cdp-release-prometheus-kube-state-metrics-9d948fc4-24s9g          3m           31Mi            
cdp-release-prometheus-server-c9fdd6947-sgttg                     28m          275Mi           
cdp-release-reloader-8d4c78b5f-rjzhh                              0m           14Mi            
cdp-release-resource-pool-manager-7b59c9fcd-z2g99                 7m           33Mi            
cdp-release-thunderhead-cdp-private-authentication-consolecqvh6   0m           33Mi            
cdp-release-thunderhead-cdp-private-commonconsole-7494dbdb55xtf   0m           33Mi            
cdp-release-thunderhead-cdp-private-environments-console-9qd69r   0m           33Mi            
cdp-release-thunderhead-compute-api-759fcfdfb5-lt8jr              2m           343Mi           
cdp-release-thunderhead-consoleauthenticationcdp-9ff4fc4cfnghvv   2m           386Mi           
cdp-release-thunderhead-consoleauthenticationcdp-dd57f7566mg878   203m         346Mi           
cdp-release-thunderhead-de-api-5b45f56bf4-vgkc5                   1m           316Mi           
cdp-release-thunderhead-dw-api-7849dd578d-49h6q                   2m           320Mi           
cdp-release-thunderhead-environment-776446b6cf-phpvp              29m          579Mi           
cdp-release-thunderhead-environments2-api-68fbb6986c-27j8n        1m           358Mi           
cdp-release-thunderhead-iam-api-57df547c4d-nw4sm                  2m           315Mi           
cdp-release-thunderhead-iam-console-5cd9b684d8-js7qb              0m           33Mi            
cdp-release-thunderhead-kerberosmgmt-api-6cb4756657-dr9cd         2m           326Mi           
cdp-release-thunderhead-ml-api-555b86c6d8-b2lpb                   4m           334Mi           
cdp-release-thunderhead-resource-management-console-f7c5dfx96sf   0m           32Mi            
cdp-release-thunderhead-sdx2-api-77f89bf857-snq8f                 3m           315Mi           
cdp-release-thunderhead-servicediscoverysimple-78ff5f645d-tdpl5   5m           259Mi           
cdp-release-thunderhead-usermanagement-private-69c9d549c6-9j8h4   16m          583Mi           
dp-mlx-control-plane-app-599c66b8bb-6s62f                         0m           47Mi            
dp-mlx-control-plane-app-5cd7b9f7fd-xvwdp                         0m           52Mi            
dp-mlx-control-plane-app-health-poller-7f76fcf85b-lf8zh           4m           43Mi            
fluentd-aggregator-0                                              3m           407Mi           
snmp-notifier-94dcc6757-fcqb8                                     0m           28Mi 

[root@ocpbastion ~]# oc adm top pods -n yunikorn
NAME                                             CPU(cores)   MEMORY(bytes)   
yunikorn-admission-controller-6f66497555-p66cb   6m           9Mi             
yunikorn-scheduler-8695485f76-9p6fq              798m         165Mi   


[root@ocpbastion ~]# oc -n cdp get pvc
NAME                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
cdp-embedded-db-backend                                Bound    pvc-02485af0-181a-44e9-b642-fd8137bb8cc8   200Gi      RWO            ocs-storagecluster-ceph-rbd   9m21s
cdp-release-prometheus-server                          Bound    pvc-bda07d59-ac14-46e3-aeb2-cb29b0038077   10Gi       RWO            ocs-storagecluster-ceph-rbd   8m20s
logs                                                   Bound    pvc-21e6f1ee-1afd-46dc-8c14-405fa41559a1   20Gi       RWO            ocs-storagecluster-ceph-rbd   8m19s
storage-volume-cdp-release-prometheus-alertmanager-0   Bound    pvc-88319db3-a1a2-4e40-953a-eeffe9ed4a36   2Gi        RWO            ocs-storagecluster-ceph-rbd   8m20s
storage-volume-cdp-release-prometheus-alertmanager-1   Bound    pvc-9a736d2c-b17d-48f8-b1d5-d3642a5a42d4   2Gi        RWO            ocs-storagecluster-ceph-rbd   5m30s


[root@ocpbastion ~]# oc -n yunikorn get pvc
No resources found in yunikorn namespace.


[root@ocpbastion ~]# oc -n cdp-env-1-4e7b52f8-monitoring-platform adm top pods
NAME                                                        CPU(cores)   MEMORY(bytes)   
monitoring-cm-health-exporter-584bfb847c-dpc87              1m           329Mi           
monitoring-logger-alert-receiver-5477f8855d-nkgh6           1m           134Mi           
monitoring-metrics-server-exporter-64bddc795c-wl2sh         2m           136Mi           
monitoring-platform-proxy-5999c48944-m4l8p                  1m           109Mi           
monitoring-prometheus-alertmanager-0                        3m           121Mi           
monitoring-prometheus-kube-state-metrics-59d4b9c68f-2ldrr   1m           112Mi           
monitoring-prometheus-pushgateway-5b96d67bd-nq6b5           0m           111Mi           
monitoring-prometheus-server-6c79698487-xdv8c               3m           152Mi           
snmp-notifier-858947f855-hkslp                              0m           106Mi           
[root@ocpbastion ~]# oc -n cdp-env-1-4e7b52f8-monitoring-platform get pvc
NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
monitoring-prometheus-server                          Bound    pvc-fdfce367-ac26-41c3-a3d2-5246811fd0b7   60Gi       RWO            ocs-storagecluster-ceph-rbd   101s
storage-volume-monitoring-prometheus-alertmanager-0   Bound    pvc-091d2e29-8b56-4c4a-9875-bf8491ae52c2   2Gi        RWO            ocs-storagecluster-ceph-rbd   101s
storage-volume-monitoring-prometheus-alertmanager-1   Bound    pvc-59eb8e0d-deea-4c9d-992e-5982dccd6ac9   2Gi        RWO            ocs-storagecluster-ceph-rbd   55s





ECS:
[root@ecsmaster1 ~]# oc -n vault-system get pvc
NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-vault-0   Bound    pvc-46799d76-693f-4c16-a9e0-2ee596bb0d50   10Gi       RWO            longhorn       78m

[root@ecsmaster1 ~]# oc get pvc -n default-57478ad3-monitoring-platform
NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
monitoring-prometheus-server                          Bound    pvc-10bb1924-df91-40df-87b5-3bd6b87ff390   60Gi       RWO            longhorn       66m
storage-volume-monitoring-prometheus-alertmanager-0   Bound    pvc-9410b27f-ca24-4947-95b3-a522b2c12e1e   2Gi        RWO            longhorn       66m
storage-volume-monitoring-prometheus-alertmanager-1   Bound    pvc-3ea29318-3b48-42aa-bf89-bcb59a053e15   2Gi        RWO            longhorn       65m

[root@ecsmaster1 ~]# oc get pvc -n default
No resources found in default namespace.
[root@ecsmaster1 ~]# oc get pvc -n cdp
NAME                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cdp-embedded-db-backend                                Bound    pvc-b9d0fe18-df39-424e-b2c6-0958d06d48a7   200Gi      RWO            longhorn       75m
cdp-release-prometheus-server                          Bound    pvc-c26e6bde-a524-4dd2-a28f-6b4d4bf96958   10Gi       RWO            longhorn       74m
logs                                                   Bound    pvc-3709ac3d-e2b9-480f-add5-48b910f0c308   20Gi       RWO            longhorn       74m
storage-volume-cdp-release-prometheus-alertmanager-0   Bound    pvc-e85913d3-d0b4-4779-8678-8244320bdb71   2Gi        RWO            longhorn       74m
storage-volume-cdp-release-prometheus-alertmanager-1   Bound    pvc-e577f155-9618-40a0-9b7b-53127c2275bb   2Gi        RWO            longhorn       72m

[root@ecsmaster1 ~]# oc get pvc -n infra-prometheus
NAME                                                                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
prometheus-infra-prometheus-operator-prometheus-db-prometheus-infra-prometheus-operator-prometheus-0   Bound    pvc-c0566ce6-59b6-4ddd-9447-4c9ccee3cce1   10Gi       RWO            longhorn       76m

[root@ecsmaster1 ~]# oc get pvc -n cdp
NAME                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cdp-embedded-db-backend                                Bound    pvc-b9d0fe18-df39-424e-b2c6-0958d06d48a7   200Gi      RWO            longhorn       76m
cdp-release-prometheus-server                          Bound    pvc-c26e6bde-a524-4dd2-a28f-6b4d4bf96958   10Gi       RWO            longhorn       75m
logs                                                   Bound    pvc-3709ac3d-e2b9-480f-add5-48b910f0c308   20Gi       RWO            longhorn       75m
storage-volume-cdp-release-prometheus-alertmanager-0   Bound    pvc-e85913d3-d0b4-4779-8678-8244320bdb71   2Gi        RWO            longhorn       75m
storage-volume-cdp-release-prometheus-alertmanager-1   Bound    pvc-e577f155-9618-40a0-9b7b-53127c2275bb   2Gi        RWO            longhorn       73m

[root@ecsmaster1 ~]# oc -n impala-1658935473-x95q  get pvc
NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
scratch-cache-volume-coordinator-0           Bound    pvc-e4230d33-9501-4ac3-865a-1ba93bfc7d7b   100Gi      RWO            local-path     2m18s
scratch-cache-volume-coordinator-1           Bound    pvc-56d7fe8a-bf3d-44ad-91d1-2e7472cb7156   100Gi      RWO            local-path     104s
scratch-cache-volume-impala-executor-000-0   Bound    pvc-c588db46-ad14-4867-bca3-777b29da5bf4   100Gi      RWO            local-path     2m18s
[root@ecsmaster1 ~]# oc -n impala-1658935473-x95q  get pods
NAME                                READY   STATUS    RESTARTS   AGE
catalogd-586d576644-8vjb8           1/1     Running   0          2m30s
coordinator-0                       4/4     Running   0          2m30s
coordinator-1                       3/4     Running   0          116s
huebackend-0                        2/2     Running   0          2m30s
huefrontend-677854c967-44h5q        1/1     Running   0          2m30s
impala-autoscaler-b79d84f47-mknbg   2/2     Running   0          2m30s
impala-executor-000-0               1/1     Running   0          2m30s
statestored-b68797948-x4tbx         1/1     Running   0          2m30s
usage-monitor-6945465658-6cr2d      1/1     Running   0          2m30s

[root@ecsmaster1 ~]# oc -n compute-1658937812-qwff  get pods
NAME                             READY   STATUS    RESTARTS   AGE
das-webapp-0                     1/1     Running   0          116s
hiveserver2-0                    1/1     Running   0          116s
huebackend-0                     1/1     Running   0          116s
huefrontend-64f56d476b-94mgg     1/1     Running   0          116s
query-coordinator-0-0            1/1     Running   0          110s
query-executor-0-0               1/1     Running   0          110s
standalone-compute-operator-0    2/2     Running   0          116s
usage-monitor-566677d48b-gbwrd   1/1     Running   0          116s
[root@ecsmaster1 ~]# oc -n compute-1658937812-qwff  get pvc
NAME                                                         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                        AGE
compute-1658937812-qwff--pvc                                 Pending                                                                        compute-1658937812-qwff-azurefile   2m6s
query-executor-62e161f2-365a8581-volume-query-executor-0-0   Bound     pvc-b1517f92-60f8-448a-bb2f-c9faf454b2d6   100Gi      RWO            local-path                          2m


[root@ocpbastion ~]# oc -n ws2 get pvc #external nfs
NAME                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
livelog-data-livelog-0                  Bound    pvc-e7be2f9a-370c-4606-a917-c6248630237b   100Gi      RWO            ocs-storagecluster-ceph-rbd   5m45s
model-metrics-data-model-metrics-db-0   Bound    pvc-ccc07720-5ab4-4ca7-a5e6-ecc7c268b2d3   50Gi       RWO            ocs-storagecluster-ceph-rbd   5m45s
persist-dir-secret-generator-0          Bound    pvc-ecc742c5-84de-4289-9a3f-39972adde54a   10Mi       RWO            ocs-storagecluster-ceph-rbd   5m46s
postgres-data-versioned-db-0            Bound    pvc-31cc4c72-a007-462f-8a7e-ecb69fb4ced7   50Gi       RWO            ocs-storagecluster-ceph-rbd   5m45s
projects-pvc                            Bound    projects-share-ws2                         1Ti        RWX                                          5m46s
registry-pvc                            Bound    pvc-774e35b5-2656-41f0-a805-786ce1d2389c   250Gi      RWO            ocs-storagecluster-ceph-rbd   5m46s
s2i-git-server-repos-s2i-git-server-0   Bound    pvc-cd187d88-3e4c-4835-a9cd-9112392cdbd9   100Gi      RWO            ocs-storagecluster-ceph-rbd   5m45s
s2i-queue-pvc                           Bound    pvc-a644b10e-f219-469a-a653-fd0da66ecbd5   2Gi        RWO            ocs-storagecluster-ceph-rbd   5m46s


[root@ecsmaster1 ~]# kubectl -n workspace1 get pvc #internal nfs
NAME                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                 AGE
livelog-data-livelog-0                  Bound    pvc-f5f9f9dc-64a4-40af-a834-3ba528cdf233   100Gi      RWO            longhorn                     89s
longhorn-nfs-workspace1                 Bound    pvc-08725737-3a88-4b52-b7c4-d2eca1f09b20   476838Mi   RWO            longhorn                     89s
model-metrics-data-model-metrics-db-0   Bound    pvc-c4122fa3-de7b-46b1-91c6-c3fb2df6017e   50Gi       RWO            longhorn                     89s
persist-dir-secret-generator-0          Bound    pvc-db799c64-98c9-4044-8379-8fc8fde52bd9   10Mi       RWO            longhorn                     89s
postgres-data-versioned-db-0            Bound    pvc-113f07ab-18ea-49be-94e9-b5bdb8076fce   50Gi       RWO            longhorn                     89s
projects-pvc                            Bound    pvc-57bea15d-6778-47e3-8008-d3fe276044de   10Gi       RWX            longhorn-nfs-sc-workspace1   89s
s2i-git-server-repos-s2i-git-server-0   Bound    pvc-6c3c1de2-5a8c-4f24-84d4-c41486bbeb37   100Gi      RWO            longhorn                     89s
s2i-queue-pvc                           Bound    pvc-5005467b-ca31-4d44-995c-0982a3a8bf76   2Gi        RWO            longhorn                     89s


[root@ecsmaster1 ~]# oc get sc
NAME                         PROVISIONER              RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path                   rancher.io/local-path    Delete          WaitForFirstConsumer   false                  121m
longhorn (default)           driver.longhorn.io       Delete          Immediate              true                   120m
longhorn-nfs                 nfs.longhorn.io          Delete          Immediate              false                  120m
longhorn-nfs-sc-workspace1   longhorn-pv-workspace1   Delete          Immediate              false                  100s
[root@ecsmaster1 ~]# oc -n workspace1 get pods
NAME                                             READY   STATUS      RESTARTS   AGE
api-78866f689-hr8xj                              1/1     Running     3          5m34s
cron-56d98f7f5b-mw4mw                            2/2     Running     0          5m34s
db-0                                             2/2     Running     0          5m34s
db-migrate-2.0.31-b62-r8v4j                      0/1     Completed   0          5m34s
ds-cdh-client-764f888d95-p268m                   3/3     Running     0          5m33s
ds-operator-97c84ff9b-92mgl                      2/2     Running     1          5m32s
ds-reconciler-6d79d8cf57-7vblf                   2/2     Running     2          5m34s
ds-vfs-6478cc94f6-6pzsq                          2/2     Running     0          5m33s
feature-flags-5449b955cf-rwg9d                   2/2     Running     0          5m33s
fluentd-forwarder-6bd676b76d-b8htl               1/1     Running     0          5m33s
governance-6dd447b9ff-jhmns                      2/2     Running     0          5m32s
hadoop-cli-7.2.10-hf1-ehby3-7gcds                1/1     Running     0          2m10s
hadoop-cli-7.2.11-hf4-mdxxr-l2cv5                1/1     Running     0          2m10s
hadoop-cli-7.2.14-6kdvv-b65td                    1/1     Running     0          2m10s
hadoop-cli-7.2.8-hf1-1m6xc9-xxv2p                0/1     Completed   0          2m10s
livelog-0                                        2/2     Running     0          5m34s
livelog-publisher-hq575                          2/2     Running     2          5m34s
livelog-publisher-lw5s4                          2/2     Running     0          5m34s
livelog-publisher-thk6f                          2/2     Running     1          5m34s
longhorn-nfs-workspace1-85bc5c99d-hmkcz          1/1     Running     0          5m34s
mlx-workspace1-pod-evaluator-54dbdc7d68-8khf9    1/1     Running     0          5m34s
model-metrics-6456cfcb45-x4bdt                   1/1     Running     4          5m34s
model-metrics-db-0                               1/1     Running     0          5m34s
model-proxy-55687bfc6d-bd2dx                     2/2     Running     0          5m34s
prometheus-postgres-exporter-68b68d9b9b-466dj    1/1     Running     0          5m34s
runtime-addon-trigger-2.0.31-b62-nwv2r           1/1     Running     2          5m34s
runtime-initial-repo-inserter-2.0.31-b62-zgcgh   0/1     Completed   0          5m34s
runtime-manager-6f9864b45b-l8tfq                 2/2     Running     0          5m34s
s2i-builder-8458db65df-bfs6k                     2/2     Running     0          5m32s
s2i-builder-8458db65df-j89fm                     2/2     Running     1          5m32s
s2i-builder-8458db65df-l7hw5                     2/2     Running     0          5m32s
s2i-client-dcf86964b-g2sql                       2/2     Running     1          5m34s
s2i-git-server-0                                 2/2     Running     0          5m34s
s2i-queue-0                                      2/2     Running     0          5m34s
s2i-server-846b48fcfd-7fkp2                      2/2     Running     0          5m32s
secret-generator-0                               2/2     Running     0          5m34s
spark248-15-hf1-i807cc-4btkk                     1/1     Running     0          2m10s
spark320-15-hf2-5mvybf-jmgk2                     1/1     Running     0          2m10s
tcp-ingress-controller-675f457d4c-jtlq7          2/2     Running     2          5m34s
usage-reporter-7bb5676f-f7fcs                    2/2     Running     0          5m34s
web-947c48488-cs5gc                              2/2     Running     0          5m34s
web-947c48488-hqw5h                              1/2     Running     0          5m34s
web-947c48488-ml5xj                              1/2     Running     0          5m34s

[root@ecsmaster1 ~]# oc -n viz-1658961242-rnrm  get pods
NAME                                READY   STATUS      RESTARTS   AGE
usage-monitor-7c7b695d7c-dzp6r      1/1     Running     0          2m23s
viz-webapp-0                        2/2     Running     0          2m23s
viz-webapp-vizdb-create-job-9jxjr   0/1     Completed   0          2m26s
[root@ecsmaster1 ~]# oc -n viz-1658961242-rnrm  get pvc
No resources found in viz-1658961242-rnrm namespace.


[root@ecsmaster1 ~]# oc -n compute-1658968106-57bw get pvc
NAME                                                         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                        AGE
compute-1658968106-57bw--pvc                                 Pending                                                                        compute-1658968106-57bw-azurefile   5m44s
query-executor-62e1d849-57e9d186-volume-query-executor-0-0   Bound     pvc-84a64724-5a39-4a7b-bc39-fe1f7eecde38   100Gi      RWO            local-path                          5m38s
query-executor-62e1d849-57e9d186-volume-query-executor-0-1   Bound     pvc-1735b05c-0e78-40a2-918b-4cfffd4dca30   100Gi      RWO            local-path                          5m38s
query-executor-62e1d849-57e9d186-volume-query-executor-0-2   Bound     pvc-21846660-6e48-4c37-98ec-962a5468a49a   100Gi      RWO            local-path                          5m38s
[root@ecsmaster1 ~]# oc -n compute-1658968106-57bw get pods
NAME                            READY   STATUS    RESTARTS   AGE
das-webapp-0                    1/1     Running   0          5m50s
hiveserver2-0                   1/1     Running   0          5m49s
huebackend-0                    1/1     Running   0          5m49s
huefrontend-6f4f99dd74-mj9m5    1/1     Running   0          5m50s
query-coordinator-0-0           1/1     Running   0          5m43s
query-coordinator-0-1           1/1     Running   0          5m43s
query-coordinator-0-2           1/1     Running   0          5m43s
query-executor-0-0              1/1     Running   0          5m43s
query-executor-0-1              1/1     Running   0          5m43s
query-executor-0-2              1/1     Running   0          5m43s
standalone-compute-operator-0   2/2     Running   0          5m50s
usage-monitor-b95fcf4f-xfl7c    1/1     Running   0          5m49s


Impala noHA LITE 1 executor:
[root@ecsmaster1 ~]# oc -n impala-1659072064-h2xz get pvc
NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
scratch-cache-volume-coordinator-0           Bound    pvc-5e7a03c3-77e4-456a-b7c7-5f3e4e7d8177   100Gi      RWO            local-path     61s
scratch-cache-volume-impala-executor-000-0   Bound    pvc-ffbda4a1-8501-4884-8656-a19fbefb9b77   100Gi      RWO            local-path     61s

[root@ecsmaster1 ~]# oc -n impala-1659072064-h2xz get pods
NAME                             READY   STATUS        RESTARTS   AGE
catalogd-768ffc8d78-9xdpc        1/1     Running       0          2m52s
coordinator-0                    4/4     Running       0          2m52s
huebackend-0                     2/2     Running       0          2m52s
huefrontend-57dbfdb85c-4bkk7     1/1     Running       0          2m52s
impala-executor-000-0            1/1     Running       0          2m52s
statestored-56fbb6597d-4tfxz     1/1     Running       0          2m52s
usage-monitor-675cf694f8-2jp4z   1/1     Running       3          2m52s


Impala noHA LITE 2 executor:
[root@ecsmaster1 ~]# oc -n impala-1659073033-rgq9 get pods
NAME                                READY   STATUS             RESTARTS   AGE
catalogd-864dcbdbcc-7cxpc           1/1     Running            0          2m36s
coordinator-0                       4/4     Running            0          2m36s
huebackend-0                        2/2     Running            0          2m36s
huefrontend-868b48dbcd-p7x74        1/1     Running            0          2m36s
impala-autoscaler-9f9fb4dbb-4dvvc   2/2     Running            0          2m36s
impala-executor-000-0               1/1     Running            0          2m36s
impala-executor-000-1               1/1     Running            0          2m36s
statestored-848c7898b5-gxx4q        1/1     Running            0          2m36s
usage-monitor-5fcf4bbc8-jcsv7       0/1     CrashLoopBackOff   2          2m36s
[root@ecsmaster1 ~]# oc -n impala-1659073033-rgq9 get pvc
NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
scratch-cache-volume-coordinator-0           Bound    pvc-9248d1d1-678a-4851-a64e-482f2c3a700e   100Gi      RWO            local-path     2m38s
scratch-cache-volume-impala-executor-000-0   Bound    pvc-f9458b6b-586f-46d6-bfd2-49a6ed8afcbf   100Gi      RWO            local-path     2m38s
scratch-cache-volume-impala-executor-000-1   Bound    pvc-477dc389-6627-458f-957b-a62a976db9b8   100Gi      RWO            local-path     2m38s

Impala noHA LITE 3 executor:
[root@ecsmaster1 ~]# oc -n impala-1659073349-jw7w get pods
NAME                                READY   STATUS    RESTARTS   AGE
catalogd-6cc48cdcbd-ddjxq           1/1     Running   0          2m8s
coordinator-0                       4/4     Running   0          2m8s
huebackend-0                        2/2     Running   0          2m8s
huefrontend-7f54c7c945-fn5n8        1/1     Running   0          2m8s
impala-autoscaler-dcdcdbd79-5pkxn   2/2     Running   0          2m8s
impala-executor-000-0               1/1     Running   0          2m8s
impala-executor-000-1               1/1     Running   0          2m8s
impala-executor-000-2               1/1     Running   0          2m8s
statestored-cb4f598dd-v5gmc         1/1     Running   0          2m8s
usage-monitor-776749795-2hvb6       1/1     Running   2          2m8s
[root@ecsmaster1 ~]# oc -n impala-1659073349-jw7w get pvc
NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
scratch-cache-volume-coordinator-0           Bound    pvc-8cb0f6ff-2ee1-4aa9-8bdf-0d8ba0af50db   100Gi      RWO            local-path     2m10s
scratch-cache-volume-impala-executor-000-0   Bound    pvc-3ec795a6-01cd-4fd9-93b9-f89ed37b285b   100Gi      RWO            local-path     2m10s
scratch-cache-volume-impala-executor-000-1   Bound    pvc-66654e86-0490-4432-aadb-17f2149762e1   100Gi      RWO            local-path     2m10s
scratch-cache-volume-impala-executor-000-2   Bound    pvc-3cf6cb43-d26d-4ac4-acb2-7d688fde167b   100Gi      RWO            local-path     2m10s


Impala HA LITE 1 executor:
[root@ecsmaster1 ~]# oc -n impala-1659073825-rkcl get pods
NAME                                 READY   STATUS             RESTARTS   AGE
catalogd-67bfbcdff4-l4mkc            1/1     Running            0          4m12s
coordinator-0                        4/4     Running            0          4m12s
coordinator-1                        3/4     Running            0          3m32s
huebackend-0                         2/2     Running            0          4m12s
huefrontend-858b48985f-dk8pj         1/1     Running            0          4m12s
impala-autoscaler-8488987bd5-8grlh   2/2     Running            0          4m12s
impala-executor-000-0                1/1     Running            0          4m12s
statestored-675754c9d6-dg4s6         1/1     Running            0          4m12s
usage-monitor-5b999bc786-w8sb4       0/1     CrashLoopBackOff   4          4m12s
[root@ecsmaster1 ~]# oc -n impala-1659073825-rkcl get pvc
NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
scratch-cache-volume-coordinator-0           Bound    pvc-d8685e78-cad0-41f9-80d2-d20e0b6503ea   100Gi      RWO            local-path     4m20s
scratch-cache-volume-coordinator-1           Bound    pvc-bab1573a-e5d6-4925-8e97-c917976a7067   100Gi      RWO            local-path     3m40s
scratch-cache-volume-impala-executor-000-0   Bound    pvc-a710b30f-0cab-4c33-817d-f7eed9166d9f   100Gi      RWO            local-path     4m20s





I0727 15:15:18.275321    67 RequestPoolService.java:163] Loading configuration: /opt/impala/conf/llama-site.xml
I0727 15:15:18.276790     1 mem-info.cc:224] System memory available: 251.74 GB (from physical mem)
I0727 15:15:18.276998     1 mem-info.cc:231] CGroup memory limit for this process reduces physical memory available to: 11.44 GB
I0727 15:15:18.277015     1 mem-info.cc:248] Using process memory limit: 9.16 GB (--mem_limit=80% of 11.44 GB)
I0727 15:15:18.284292     1 status.cc:129] Invalid combination of --mem_limit_includes_jvm and JVM max heap size 26808680448, which must be smaller than process memory limit 9830400000
    @           0xd99193
    @          0x11dd364
    @          0x13f92f4
    @           0xbd6efa
    @     0x7fea1a104554
    @           0xc7ba86
F0727 15:15:18.284696     1 impalad-main.cc:72] Invalid combination of --mem_limit_includes_jvm and JVM max heap size 26808680448, which must be smaller than process memory limit 9830400000
. Impalad exiting.
*** Check failure stack trace: ***
    @          0x319747c
    @          0x3198d2c
    @          0x3196dda
    @          0x319a998
    @          0x13f999e
    @           0xbd6efa
    @     0x7fea1a104554
    @           0xc7ba86
    
    
I0727 14:50:52.589313     1 status.cc:129] LDAP authentication specified, but without TLS. Passwords would go over the network in the clear. Enable TLS with --ldap_tls or use an ldaps:// URI. To override this is non-production environments, specify --ldap_passwords_in_clear_ok
    @           0xd99193
    @          0x1653a6a
    @          0x1654c7b
    @          0x1653b54
    @          0x116cf60
    @          0x116dae0
    @           0xd8fc51
    @           0xc85a73
    @           0xbd6fd0
    @     0x7fa85a465554
    @           0xc7ba86
F0727 14:50:52.589797     1 init.cc:360] LDAP authentication specified, but without TLS. Passwords would go over the network in the clear. Enable TLS with --ldap_tls or use an ldaps:// URI. To override this is non-production environments, specify --ldap_passwords_in_clear_ok
. Impalad exiting.
*** Check failure stack trace: ***
    @          0x319747c
    @          0x3198d2c
    @          0x3196dda
    @          0x319a998
    @           0xd90c5e
    @           0xc85a73
    @           0xbd6fd0
    @     0x7fa85a465554
    @           0xc7ba86




[root@ecsworker1 ~]# top #before DS installation

top - 13:57:53 up 17:30,  2 users,  load average: 8.55, 8.44, 8.64
Tasks: 605 total,   2 running, 603 sleeping,   0 stopped,   0 zombie
%Cpu(s):  9.1 us,  4.7 sy,  0.0 ni, 85.9 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st
KiB Mem : 26397244+total, 20122844+free,  9492216 used, 53251784 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 25313662+avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                  
29624 root      20   0  764592  83708   6908 S 103.7  0.0 834:16.12 ruby                                                                                     
 1797 root      20   0 1464436 121164   6884 S  83.4  0.0  66:16.50 ruby                                                                                     
30353 root      20   0  926560 125076  38264 S  21.9  0.0 215:03.68 containerd                                                                               
29660 root      20   0    4196    728    444 S  15.0  0.0 120:48.62 sh                                                                                       
29795 centos    20   0    4196    736    452 R  13.6  0.0 119:28.64 sh                                                                                       
28457 root      20   0  717116  16296   5548 S  11.6  0.0 104:11.25 containerd-shim                                                                          
30570 root      20   0  977872 165028  40256 S  10.0  0.1 145:30.75 kubelet                                                                                  
  788 root      20   0 3584320  67060  22032 S   3.3  0.0  21:24.62 calico-node                                                                              
 8996 root      20   0 9388324  93632   9892 S   1.7  0.0  15:33.56 python2                                                                                  
10301 root      20   0 3273512  88216  21072 S   1.3  0.0  75:40.29 dockerd                                                                                  
    1 root      20   0   46480   6908   2620 S   0.7  0.0  11:55.27 systemd                                                                                  
    9 root      20   0       0      0      0 S   0.7  0.0   5:05.23 rcu_sched                                                                                
 1393 root      20   0  717116  17516   5900 S   0.7  0.0   3:40.85 containerd-shim                                                                          
19526 root      20   0  173344   2884   1632 R   0.7  0.0   0:00.09 top                                                                                      
29347 1001      20   0  482288  42084   5916 S   0.7  0.0   1:44.95 ruby                                                                                     
30984 root      20   0  717116  17356   5860 S   0.7  0.0   2:58.43 containerd-shim                                                                          
    6 root      20   0       0      0      0 S   0.3  0.0   3:45.12 ksoftirqd/0                                                                              
   33 root      rt   0       0      0      0 S   0.3  0.0   0:27.22 migration/5                                                                              
  108 root      rt   0       0      0      0 S   0.3  0.0   0:29.43 migration/20                                                                             
  113 root      rt   0       0      0      0 S   0.3  0.0   0:28.82 migration/21                                                                             
  153 root      rt   0       0      0      0 S   0.3  0.0   0:28.91 migration/29                                                                             
 1003 root      20   0  748536  31916  12864 S   0.3  0.0   1:02.16 flanneld                                                                                 
 1600 root      20   0  762632  47544  18292 S   0.3  0.0   6:25.28 coredns                                                                                  
 1819 root      20   0  820164 101716  18476 S   0.3  0.0  11:02.97 longhorn-manage                                                                          
 8955 root      20   0 2329124  81064  10260 S   0.3  0.0   8:54.21 cmagent                                                                                  
 9581 root      20   0  709824  64016  10688 S   0.3  0.0   6:13.93 python2                                                                                  
10734 nfsnobo+  20   0  722172  23964   6476 S   0.3  0.0   7:23.79 node_exporter                                                                            
10781 nfsnobo+  20   0  749048  58760  14872 S   0.3  0.0   3:41.63 operator                                                                                 
16475 1001      20   0 2704844 108788  35048 S   0.3  0.0   2:14.42 alert-admin-ser                                                                          
17012 1001      20   0  780772  56340  23352 S   0.3  0.0   4:44.47 traefik                                                                                  
21790 1001      20   0 2851304 108068  34740 S   0.3  0.0   2:34.61 monitoring-pvcs                                                                          
28453 1001      20   0 2408092  84980  29768 S   0.3  0.0   1:48.28 metrics-server-                                                                          
28455 1001      20   0 2334320  73332  27760 S   0.3  0.0   0:30.13 logger-alert-re                                                                          
29197 1001      20   0  414660  39272   5988 S   0.3  0.0   0:20.25 td-agent                                                                                 
29330 1001      20   0  481656  41788   5956 S   0.3  0.0   1:39.52 ruby                                                                                     
29346 1001      20   0  482280  42848   5956 S   0.3  0.0   1:43.22 ruby                                                                                     
29352 1001      20   0  483592  43064   5976 S   0.3  0.0   1:42.77 ruby                                                                                     
29878 1001      20   0 1069496 628348  24884 S   0.3  0.2  35:10.29 prometheus                                                                               
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.05 kthreadd                                                                                 
    4 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H                                                                             
    7 root      rt   0       0      0      0 S   0.0  0.0   0:20.97 migration/0                                                                              
[root@ecsworker1 ~]# ps aux | grep ruby
root      1797 72.8  0.0 1464476 127400 ?      Sl   12:23  76:04 /usr/local/bin/ruby -Eascii-8bit:ascii-8bit /usr/local/bin/fluentd -c /fluentd/etc/fluent.conf -p /fluentd/plugins --under-supervisor
1001     29197  0.0  0.0 414660 39272 ?        Ssl  Jul27   0:20 /opt/td-agent/bin/ruby /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf
1001     29198  0.0  0.0 414188 38568 ?        Ssl  Jul27   0:20 /opt/td-agent/bin/ruby /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf
1001     29199  0.0  0.0 414668 39280 ?        Ssl  Jul27   0:20 /opt/td-agent/bin/ruby /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf
1001     29200  0.0  0.0 414608 39260 ?        Ssl  Jul27   0:20 /opt/td-agent/bin/ruby /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf
1001     29330  0.1  0.0 481656 41788 ?        Sl   Jul27   1:40 /opt/td-agent/bin/ruby -Eascii-8bit:ascii-8bit /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf --under-supervisor
1001     29346  0.1  0.0 482280 42848 ?        Sl   Jul27   1:44 /opt/td-agent/bin/ruby -Eascii-8bit:ascii-8bit /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf --under-supervisor
1001     29347  0.1  0.0 482288 42084 ?        Sl   Jul27   1:46 /opt/td-agent/bin/ruby -Eascii-8bit:ascii-8bit /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf --under-supervisor
1001     29352  0.1  0.0 483592 43324 ?        Sl   Jul27   1:43 /opt/td-agent/bin/ruby -Eascii-8bit:ascii-8bit /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf --under-supervisor
root     29624 91.4  0.0 766552 85484 ?        Ssl  Jul27 844:47 ruby /usr/local/bin/fluentd -c /fluentd/etc/fluent.conf -p /fluentd/plugins
1001     29918  0.0  0.0 414612 39280 ?        Ssl  Jul27   0:19 /opt/td-agent/bin/ruby /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf
1001     30019  0.1  0.0 482308 42340 ?        Sl   Jul27   1:43 /opt/td-agent/bin/ruby -Eascii-8bit:ascii-8bit /usr/sbin/td-agent -c /etc/td-agent/td-agent.conf --under-supervisor
root     32555  0.0  0.0 112788  1008 pts/0    S+   14:08   0:00 grep --color=auto ruby





[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf describe pod das-webapp-0  | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf describe pod hiveserver2-0  | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf describe pod huebackend-0  | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf describe pod huefrontend-944df7889-mkt6d | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf describe pod query-coordinator-0-0  | grep -i node
Node:         ecsworker1.cdpkvm.cldr/10.15.4.167
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf describe pod query-coordinator-0-1 | grep -i node
Node:         ecsworker2.cdpkvm.cldr/10.15.4.171
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf describe pod query-executor-0-0 | grep -i node
Node:         ecsworker1.cdpkvm.cldr/10.15.4.167
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf describe pod query-executor-0-1 | grep -i node
Node:         ecsworker2.cdpkvm.cldr/10.15.4.171
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf describe pod standalone-compute-operator-0  | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253395-lwbf get pvc
NAME                                                         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                        AGE
compute-1659253395-lwbf--pvc                                 Pending                                                                        compute-1659253395-lwbf-azurefile   6m45s
query-executor-62e632b3-365a8581-volume-query-executor-0-0   Bound     pvc-53fe0748-1bc9-49a9-912b-157a5ae0653f   100Gi      RWO            local-path                          6m39s
query-executor-62e632b3-365a8581-volume-query-executor-0-1   Bound     pvc-c91ae1f1-db9a-43bc-951c-809a04945cca   100Gi      RWO            local-path                          6m39s



[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod das-webapp-0 | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod hiveserver2-0 | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod huebackend-0 | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod huefrontend-6f457c6fc5-mn9x7 | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod query-coordinator-0-0 | grep -i node
Node:         ecsworker1.cdpkvm.cldr/10.15.4.167
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod query-coordinator-0-1 | grep -i node
Node:         ecsworker2.cdpkvm.cldr/10.15.4.171
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod query-coordinator-0-2 | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod query-executor-0-0 | grep -i node
Node:         ecsworker1.cdpkvm.cldr/10.15.4.167
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod query-executor-0-1 | grep -i node
Node:         ecsworker2.cdpkvm.cldr/10.15.4.171
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx describe pod query-executor-0-2 | grep -i node
Node:         ecsworker3.cdpkvm.cldr/10.15.4.175
Node-Selectors:              <none>
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@ecsmaster1 ~]# oc -n compute-1659253900-vzbx get pvc
NAME                                                         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                        AGE
compute-1659253900-vzbx--pvc                                 Pending                                                                        compute-1659253900-vzbx-azurefile   4m38s
query-executor-62e634a9-365a8581-volume-query-executor-0-0   Bound     pvc-0b3d93d4-7d41-41ed-b4b7-94bd6b0474ba   100Gi      RWO            local-path                          4m35s
query-executor-62e634a9-365a8581-volume-query-executor-0-1   Bound     pvc-6393980e-1f40-434a-992a-5d6aa81d669a   100Gi      RWO            local-path                          4m34s
query-executor-62e634a9-365a8581-volume-query-executor-0-2   Bound     pvc-66906241-41f9-493f-aaf0-d2bfc29849ed   100Gi      RWO            local-path                          4m34s

    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/master03.ocp4.cdpkvm.cldr-2.qcow2'/>
      <target dev='vdc' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x06' slot='0x00' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/master03.ocp4.cdpkvm.cldr-3.qcow2'/>
      <target dev='vdd' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x07' slot='0x00' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/master03.ocp4.cdpkvm.cldr-4.qcow2'/>
      <target dev='vde' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x08' slot='0x00' function='0x0'/>
    </disk>
    


[root@ecsmaster1 ~]# kubectl get pods --show-labels -o wide -n cdp | grep master
cdp-embedded-db-0                                                 1/1     Running     0          37m   10.42.0.33   ecsmaster1.cdpkvm.cldr   <none>           <none>            app=cdp-embedded-db,controller-revision-hash=cdp-embedded-db-db86d8d6,release=cdp-release,statefulset.kubernetes.io/pod-name=cdp-embedded-db-0
cdp-release-dwx-server-64db6db896-fbbdt                           2/2     Running     0          33m   10.42.0.48   ecsmaster1.cdpkvm.cldr   <none>           <none>            app=cdp-release-dwx-server,pod-template-hash=64db6db896,release=cdp-release
cdp-release-grafana-7564467cf7-7rsmw                              3/3     Running     0          12m   10.42.0.51   ecsmaster1.cdpkvm.cldr   <none>           <none>            app.kubernetes.io/instance=cdp-release,app.kubernetes.io/name=grafana,pod-template-hash=7564467cf7
cdp-release-prometheus-kube-state-metrics-5f7c96599c-zbpl7        2/2     Running     0          36m   10.42.0.41   ecsmaster1.cdpkvm.cldr   <none>           <none>            app=prometheus,chart=prometheus-1.0.0,component=kube-state-metrics,heritage=Helm,pod-template-hash=5f7c96599c,release=cdp-release
cdp-release-thunderhead-cdp-private-authentication-consolehhqkb   2/2     Running     0          36m   10.42.0.35   ecsmaster1.cdpkvm.cldr   <none>           <none>            app=thunderhead-cdp-private-authentication-console,pod-template-hash=f7d7f6b74,release=cdp-release
cdp-release-thunderhead-cdp-private-environments-console-7chxsq   2/2     Running     0          36m   10.42.0.36   ecsmaster1.cdpkvm.cldr   <none>           <none>            app=thunderhead-cdp-private-environments-console,pod-template-hash=7b7fdf977b,release=cdp-release
cdp-release-thunderhead-de-api-666669d55c-bfsfd                   2/2     Running     0          36m   10.42.0.39   ecsmaster1.cdpkvm.cldr   <none>           <none>            app=thunderhead-de-api,pod-template-hash=666669d55c,release=cdp-release
cdp-release-thunderhead-environment-57dddc8b55-rgtcz              2/2     Running     0          33m   10.42.0.47   ecsmaster1.cdpkvm.cldr   <none>           <none>            app=thunderhead-environment,pod-template-hash=57dddc8b55,release=cdp-release
cdp-release-thunderhead-resource-management-console-7d657dg9m97   2/2     Running     0          36m   10.42.0.43   ecsmaster1.cdpkvm.cldr   <none>           <none>            app=thunderhead-resource-management-console,pod-template-hash=7d657df564,release=cdp-release
cli-mbdjq                                                         0/1     Completed   0          30m   10.42.0.49   ecsmaster1.cdpkvm.cldr   <none>           <none>            controller-uid=0559642c-99ce-4c2f-843c-b14cb36cdde1,job-name=cli


bash -c "flock -xn lockfile ls -l lockfile"
python3 -c 'import fcntl; import os; fcntl.flock(open("lockfile", "a"), fcntl.LOCK_EX | fcntl.LOCK_NB)'
python3 -c 'import fcntl; import os; fcntl.flock(os.open("lockfile", os.O_RDONLY | os.O_CREAT), fcntl.LOCK_EX | fcntl.LOCK_NB)'

bash -c "flock -xn index.html ls -l index.html"
python3 -c 'import fcntl; import os; fcntl.flock(open("index.html", "a"), fcntl.LOCK_EX | fcntl.LOCK_NB)'
python3 -c 'import fcntl; import os; fcntl.flock(os.open("index.html", os.O_RDONLY | os.O_CREAT), fcntl.LOCK_EX | fcntl.LOCK_NB)'


[root@ocpbastion ~]# oc project
Using project "ws1" on server "https://api.ocp4.cdpkvm.cldr:6443".
[root@ocpbastion ~]# for i in `oc get pods | awk '{print $1}' | grep -v NAME`; do oc get pod $i -o yaml  | grep -A2 limits: | grep memory ; done
                f:memory: {}
        memory: 512Mi
                f:memory: {}
        memory: 100Mi
                f:memory: {}
        memory: 200Mi
                f:memory: {}
        memory: 200Mi
                f:memory: {}
        memory: 1Gi
                f:memory: {}
        memory: 4Gi
                f:memory: {}
        memory: 2Gi
        memory: 1Gi
                f:memory: {}
        memory: 200Mi
                f:memory: {}
        memory: 500Mi
                f:memory: {}
        memory: 500Mi
                f:memory: {}
        memory: 500Mi
        memory: 100Mi
        memory: 1Gi
        memory: 128Mi
                f:memory: {}
        memory: 2Gi
                f:memory: {}
        memory: 2Gi
                f:memory: {}
        memory: 2Gi
                f:memory: {}
        memory: 2Gi
                f:memory: {}
        memory: 1Gi
                f:memory: {}
        memory: 2Gi
                f:memory: {}
        memory: 2Gi
                f:memory: {}
        memory: 100Mi
        memory: 2Gi
        memory: 2Gi
        memory: 2Gi
 [root@ocpbastion ~]# for i in `oc get pods | awk '{print $1}' | grep -v NAME`; do oc get pod $i -o yaml  | grep -A2 limits: | grep cpu ; done
                f:cpu: {}
        cpu: "1"
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 200m
                f:cpu: {}
        cpu: 100m
[root@ocpbastion ~]# for i in `oc get pods | awk '{print $1}' | grep -v NAME`; do oc get pod $i -o yaml  | grep -A2 requests: | grep cpu ; done
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 50m
                f:cpu: {}
                f:cpu: {}
        cpu: 200m
        cpu: 200m
                f:cpu: {}
        cpu: 50m
                f:cpu: {}
                f:cpu: {}
                f:cpu: {}
                f:cpu: {}
        cpu: 25m
        cpu: 100m
        cpu: 10m
        cpu: 25m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 500m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 200m
                f:cpu: {}
        cpu: 200m
                f:cpu: {}
        cpu: 200m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
                f:cpu: {}
        cpu: 200m
        cpu: 200m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 50m
                f:cpu: {}
        cpu: 50m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
                f:cpu: {}
        cpu: 100m
        cpu: 100m
        cpu: 100m
        cpu: 100m       
        
    5CPU 32G
    


[root@ecsmaster1 ~]# oc -n dex-app-snxg55zh get pods
NAME                                                  READY   STATUS              RESTARTS   AGE
dex-app-snxg55zh-airflow-scheduler-66d56b66c9-hwq75   1/1     Running             2          5d23h
dex-app-snxg55zh-airflow-web-5889784fdd-kdrjf         1/1     Running             0          5d23h
dex-app-snxg55zh-airflowapi-647984b4bc-6mm9x          2/2     Running             3          5d23h
dex-app-snxg55zh-api-767b8549b8-x77z7                 1/1     Running             0          3d3h
dex-app-snxg55zh-livy-584fff865c-t24cb                1/1     Running             0          3d3h
sparkwordcount-5d59318286845f0b-exec-1                2/2     Running             0          3s
sparkwordcount-5d59318286845f0b-exec-2                0/2     ContainerCreating   0          3s
sparkwordcount-5d59318286845f0b-exec-3                0/2     ContainerCreating   0          3s
testme-16-95961a82868432a1-driver                     4/4     Running             0          16s


### troubleshooting
# oc -n cdp  describe pod cdp-release-prometheus-alertmanager-0 | grep -i node 
Node:           ecsmaster1.cdpkvm.cldr/10.15.4.166
Node-Selectors:              <none>
Tolerations:                 node-role.kubernetes.io/control-plane=true:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
  Warning  FailedAttachVolume  8s    attachdetach-controller  Multi-Attach error for volume "pvc-17464fbb-be02-40c5-9990-3076cadc9754" Volume is already exclusively attached to one node and can't be attached to another
  
  
  

# GPU
yum update -y
reboot
https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#centos7
https://www.nvidia.com/Download/index.aspx?lang=en-us

yum install -y tar bzip2 make automake gcc gcc-c++ pciutils elfutils-libelf-devel libglvnd-devel vim bind-utils wget

yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

yum -y group install "Development Tools"

yum install -y kernel-devel-$(uname -r) kernel-headers-$(uname -r)

reboot

BASE_URL=https://us.download.nvidia.com/tesla
DRIVER_VERSION=515.65.01
curl -fSsl -O $BASE_URL/$DRIVER_VERSION/NVIDIA-Linux-x86_64-$DRIVER_VERSION.run
sh NVIDIA-Linux-x86_64-$DRIVER_VERSION.run

# sh NVIDIA-Linux-x86_64-$DRIVER_VERSION.run
Verifying archive integrity... OK
Uncompressing NVIDIA Accelerated Graphics Driver for Linux-x86_64 515.65.01................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................

modinfo nvidia

# nvidia-smi
Wed Aug 24 13:03:46 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  Off  | 00000000:08:00.0 Off |                    0 |
| N/A   32C    P0    37W / 250W |      0MiB / 40960MiB |      3%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

# modinfo nvidia
filename:       /lib/modules/3.10.0-1160.45.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.45.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        F2:8C:A1:ED:ED:ED:9C:7F:45:1A:98:F1:40:3A:8E:23:39:E1:E6:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp


[root@ecsgpu ~]# more /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl | grep runtime
[plugins.cri.containerd.runtimes.runc]
  runtime_type = "io.containerd.runtime.v1.linux"
  runtime = "nvidia-container-runtime"
  
  [root@ecsworker1 ~]# more /var/lib/rancher/rke2/agent/etc/containerd/config.toml | grep runtime
[plugins.cri.containerd.runtimes.runc]
  runtime_type = "io.containerd.runc.v2"
  
  
[root@ecsmaster1 ~]# oc describe node ecsgpu.cdpkvm.cldr | grep -A15 Capacity:
Capacity:
  cpu:                16
  ephemeral-storage:  209703916Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             263975200Ki
  nvidia.com/gpu:     1
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  203999969325
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             263975200Ki
  nvidia.com/gpu:     1
  pods:               110
  
[root@ecsmaster1 ~]# oc describe node ecsworker1.cdpkvm.cldr | grep -A13 Capacity:
Capacity:
  cpu:                16
  ephemeral-storage:  103797740Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             263974872Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  100974441393
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             263974872Ki
  pods:               110
  

[root@ecsmaster1 ~]# oc -n ws1-user-2 get pods 
NAME               READY   STATUS    RESTARTS   AGE
pk7m5ylf2u9ovl0j   0/5     Pending   0          3m1s
[root@ecsmaster1 ~]# oc -n ws1-user-2 describe pod pk7m5ylf2u9ovl0j | grep -B2 -i nvidia
    Limits:
      memory:          7714196Ki
      nvidia.com/gpu:  1
--
      cpu:             1960m
      memory:          7714196Ki
      nvidia.com/gpu:  1
--
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
                             nvidia.com/gpu=true:NoSchedule
--
  ----     ------            ----   ----      -------
  Warning  FailedScheduling  3m3s   yunikorn  0/4 nodes are available: 4 pod has unbound immediate PersistentVolumeClaims.
  Warning  FailedScheduling  3m2s   yunikorn  0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/control-plane: true}, that the pod didn't tolerate, 3 Insufficient nvidia.com/gpu.
  Warning  FailedScheduling  2m57s  yunikorn  0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/control-plane: true}, that the pod didn't tolerate, 3 Insufficient nvidia.com/gpu.

#########   

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

from tensorflow import keras

tf.config.experimental.list_physical_devices()
tf.test.is_built_with_cuda()

(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()

X_train.shape, X_test.shape

X_train[0].shape
y_train[:5]

X_train_scaled = X_train/255
X_test_scaled = X_test/255

y_train_encoded = keras.utils.to_categorical(y_train, num_classes = 10, dtype = 'float32')
y_test_encoded = keras.utils.to_categorical(y_test, num_classes = 10, dtype = 'float32')

def get_model():
    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(32,32,3)),
        keras.layers.Dense(3000, activation='relu'),
        keras.layers.Dense(1000, activation='relu'),
        keras.layers.Dense(10, activation='sigmoid')    
    ])
    model.compile(optimizer='SGD',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
    return model
  
%%timeit -n1 -r1
# CPU
with tf.device('/CPU:0'):
    model_cpu = get_model()
    model_cpu.fit(X_train_scaled, y_train_encoded, epochs = 10)
    

%%timeit -n1 -r1
# GPU
with tf.device('/GPU:0'):
    model_gpu = get_model()
    model_gpu.fit(X_train_scaled, y_train_encoded, epochs = 10)
    
#########    

[root@ecsgpu ~]# nvidia-smi
Thu Aug 25 13:58:40 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  Off  | 00000000:08:00.0 Off |                    0 |
| N/A   29C    P0    35W / 250W |  39185MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     29990      C   /usr/local/bin/python3.9        39183MiB |
+-----------------------------------------------------------------------------+
[root@ecsgpu ~]# ps -ef | grep 29990
root      7651  5228  0 13:59 pts/0    00:00:00 grep --color=auto 29990
8536     29990 29525 99 13:44 ?        00:39:15 /usr/local/bin/python3.9 /usr/local/bin/ipython3 kernel --automagic --no-secure --nosep --pprint --no-color-info --no-pdb --HistoryManager.enabled=False --shell=1027 --cache-size=0 --hb=1025 --colors=NoColor --iopub=1026 --user=cdsw --ident=fa566d51-2438-11ed-895a-b354d7876f97 --quiet --matplotlib=inline



## when restarting ECS node after adding node
# oc get nodes
NAME                     STATUS     ROLES                       AGE    VERSION
ecsgpu.cdpkvm.cldr       NotReady   <none>                      9m1s   v1.21.11+rke2r1
ecsmaster1.cdpkvm.cldr   NotReady   control-plane,etcd,master   5d1h   v1.21.11+rke2r1
ecsworker1.cdpkvm.cldr   NotReady   <none>                      5d1h   v1.21.11+rke2r1
ecsworker2.cdpkvm.cldr   NotReady   <none>                      5d1h   v1.21.11+rke2r1
ecsworker3.cdpkvm.cldr   NotReady   <none>                      5d1h   v1.21.11+rke2r1



## GPU Openshift
https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-nfd.html#verify-that-the-node-feature-discovery-operator-is-functioning-correctly
https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/platform-support.html



[root@ocpbastion ~]# oc get nodes
NAME                        STATUS   ROLES           AGE   VERSION
master01.ocp4.cdpkvm.cldr   Ready    master,worker   44m   v1.21.11+31d53a1
master02.ocp4.cdpkvm.cldr   Ready    master,worker   44m   v1.21.11+31d53a1
master03.ocp4.cdpkvm.cldr   Ready    master,worker   44m   v1.21.11+31d53a1
ocpgpu.ocp4.cdpkvm.cldr     Ready    worker          37m   v1.21.11+31d53a1
[root@ocpbastion ~]# oc -n nfd get pods
NAME                                      READY   STATUS    RESTARTS   AGE
nfd-controller-manager-7d6799c688-kcrpk   2/2     Running   0          13m
nfd-master-749cb                          1/1     Running   0          11m
nfd-master-7x79h                          1/1     Running   0          11m
nfd-master-q86fk                          1/1     Running   0          11m
nfd-worker-gc84j                          1/1     Running   0          11m
nfd-worker-j7gxv                          1/1     Running   0          11m
nfd-worker-vt5qb                          1/1     Running   0          11m
nfd-worker-z97x4                          1/1     Running   0          11m


[root@ocpbastion ~]# oc describe node ocpgpu.ocp4.cdpkvm.cldr | grep pci-10de.present
                    feature.node.kubernetes.io/pci-10de.present=true


[root@ocpbastion ~]# oc -n nvidia-gpu-operator get pods
NAME                                                 READY   STATUS      RESTARTS   AGE
gpu-feature-discovery-l7pqc                          1/1     Running     0          6m44s
gpu-operator-765ff6c665-mznvk                        1/1     Running     0          7m48s
nvidia-container-toolkit-daemonset-6brmr             1/1     Running     0          6m45s
nvidia-cuda-validator-8brpz                          0/1     Completed   0          2m48s
nvidia-dcgm-5txs7                                    1/1     Running     0          6m45s
nvidia-dcgm-exporter-wj4dg                           1/1     Running     0          6m44s
nvidia-device-plugin-daemonset-b2k5x                 1/1     Running     0          6m45s
nvidia-device-plugin-validator-29b9g                 0/1     Completed   0          2m30s
nvidia-driver-daemonset-48.84.202208152344-0-cxsld   2/2     Running     0          6m45s
nvidia-mig-manager-kqzk5                             1/1     Running     0          85s
nvidia-node-status-exporter-2xsdc                    1/1     Running     0          6m45s
nvidia-operator-validator-f2zdm                      1/1     Running     0          6m45s
[root@ocpbastion ~]# oc get pods,daemonset -n nvidia-gpu-operator
NAME                                                     READY   STATUS      RESTARTS   AGE
pod/gpu-feature-discovery-l7pqc                          1/1     Running     0          6m46s
pod/gpu-operator-765ff6c665-mznvk                        1/1     Running     0          7m50s
pod/nvidia-container-toolkit-daemonset-6brmr             1/1     Running     0          6m47s
pod/nvidia-cuda-validator-8brpz                          0/1     Completed   0          2m50s
pod/nvidia-dcgm-5txs7                                    1/1     Running     0          6m47s
pod/nvidia-dcgm-exporter-wj4dg                           1/1     Running     0          6m46s
pod/nvidia-device-plugin-daemonset-b2k5x                 1/1     Running     0          6m47s
pod/nvidia-device-plugin-validator-29b9g                 0/1     Completed   0          2m32s
pod/nvidia-driver-daemonset-48.84.202208152344-0-cxsld   2/2     Running     0          6m47s
pod/nvidia-mig-manager-kqzk5                             1/1     Running     0          87s
pod/nvidia-node-status-exporter-2xsdc                    1/1     Running     0          6m47s
pod/nvidia-operator-validator-f2zdm                      1/1     Running     0          6m47s

NAME                                                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                                                                                        AGE
daemonset.apps/gpu-feature-discovery                          1         1         1       1            1           nvidia.com/gpu.deploy.gpu-feature-discovery=true                                                                     6m46s
daemonset.apps/nvidia-container-toolkit-daemonset             1         1         1       1            1           nvidia.com/gpu.deploy.container-toolkit=true                                                                         6m47s
daemonset.apps/nvidia-dcgm                                    1         1         1       1            1           nvidia.com/gpu.deploy.dcgm=true                                                                                      6m47s
daemonset.apps/nvidia-dcgm-exporter                           1         1         1       1            1           nvidia.com/gpu.deploy.dcgm-exporter=true                                                                             6m46s
daemonset.apps/nvidia-device-plugin-daemonset                 1         1         1       1            1           nvidia.com/gpu.deploy.device-plugin=true                                                                             6m47s
daemonset.apps/nvidia-driver-daemonset-48.84.202208152344-0   1         1         1       1            1           feature.node.kubernetes.io/system-os_release.OSTREE_VERSION=48.84.202208152344-0,nvidia.com/gpu.deploy.driver=true   6m47s
daemonset.apps/nvidia-mig-manager                             1         1         1       1            1           nvidia.com/gpu.deploy.mig-manager=true                                                                               6m46s
daemonset.apps/nvidia-node-status-exporter                    1         1         1       1            1           nvidia.com/gpu.deploy.node-status-exporter=true                                                                      6m47s
daemonset.apps/nvidia-operator-validator                      1         1         1       1            1           nvidia.com/gpu.deploy.operator-validator=true                                                                        6m47s


[root@ocpbastion ~]# oc new-project nvidia-test
Now using project "nvidia-test" on server "https://api.ocp4.cdpkvm.cldr:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname

[root@ocpbastion ~]# cat << EOF | oc create -f -
> 
> apiVersion: v1
> kind: Pod
> metadata:
>   name: cuda-vectoradd
> spec:
>  restartPolicy: OnFailure
>  containers:
>  - name: cuda-vectoradd
>    image: "nvidia/samples:vectoradd-cuda11.2.1"
>    resources:
>      limits:
>        nvidia.com/gpu: 1
> EOF
pod/cuda-vectoradd created

[root@ocpbastion ~]# oc get pods
NAME             READY   STATUS      RESTARTS   AGE
cuda-vectoradd   0/1     Completed   0          13s

[root@ocpbastion ~]# oc logs cuda-vectoradd
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done

[root@ocpbastion ~]# oc describe pod cuda-vectoradd | grep -i Node:
Node:         ocpgpu.ocp4.cdpkvm.cldr/10.15.4.185



[root@ocpbastion ~]# oc exec -it nvidia-driver-daemonset-48.84.202208152344-0-cxsld -- nvidia-smi
Defaulted container "nvidia-driver-ctr" out of: nvidia-driver-ctr, openshift-driver-toolkit-ctr, k8s-driver-manager (init)
Fri Aug 26 06:07:36 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:07:00.0 Off |                    0 |
| N/A   28C    P0    33W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

[root@ocpbastion ~]# oc describe pod nvidia-driver-daemonset-48.84.202208152344-0-cxsld  | grep Node:
Node:                 ocpgpu.ocp4.cdpkvm.cldr/10.15.4.185

[root@ocpbastion ~]# oc -n workspace-user-1 describe pod buldb59dst035j13 | grep Node:
Node:         ocpgpu.ocp4.cdpkvm.cldr/10.15.4.185



[root@ocpbastion ~]# oc -n nvidia-gpu-operator exec -it nvidia-driver-daemonset-48.84.202208152344-0-r8rpv -- nvidia-smi
Defaulted container "nvidia-driver-ctr" out of: nvidia-driver-ctr, openshift-driver-toolkit-ctr, k8s-driver-manager (init)
Fri Aug 26 07:42:07 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:07:00.0 Off |                    0 |
| N/A   30C    P0    44W / 250W |  39326MiB / 40536MiB |     12%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A    189226      C   /usr/local/bin/python3.9        39323MiB |
+-----------------------------------------------------------------------------+


[root@ocpbastion ~]# oc adm taint node ocpgpu.ocp4.cdpkvm.cldr nvidia.com/gpu=true:NoSchedule
node/ocpgpu.ocp4.cdpkvm.cldr tainted
